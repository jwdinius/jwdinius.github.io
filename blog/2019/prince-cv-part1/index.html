<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Review of Prince's CV Book, Part One | Joe Dinius, Ph.D.</title> <meta name="author" content="Joe Dinius, Ph.D."> <meta name="description" content="Review of Part I of " computer vision. models learning and inference> <meta name="keywords" content="robotics, autonomy, math, optimization, controltheory, planning, computervision"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?53ca2be8c4ec0d533ef79017d1a2d734"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jwdinius.github.io/blog/2019/prince-cv-part1/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Joe </span>Dinius, Ph.D.</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">git</a> </li> <li class="nav-item "> <a class="nav-link" href="/learning/">learning</a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/resumeDiniusTargeted.pdf">resume</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Review of Prince's CV Book, Part One</h1> <p class="post-meta">November 9, 2019</p> <p class="post-tags"> <a href="/blog/2019"> <i class="fa-solid fa-calendar fa-sm"></i> 2019 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Over the past few months, I have been reading through Prince’s awesome book <a href="http://www.computervisionmodels.com/" rel="external nofollow noopener" target="_blank"><em>Computer Vision: Models, Learning, and Inference</em></a>. The book takes a probabilistic approach to computer vision with a remarkable level of mathematical rigor: not unapproachable, but requiring commitment to work through the details. For me, Prince’s book strikes a much-needed balance between the academic and practical concerns for practitioners of computer vision.</p> <blockquote> <p>You can find solutions to end-of-chapter exercises and algorithm implementations at my <a href="https://github.com/jwdinius/prince-computer-vision" rel="external nofollow noopener" target="_blank">GitHub repo</a>. At the time of writing this post, work through Chapter 8 has been mostly completed. I will continue adding solutions and algorithms as I review more parts, so stay tuned!</p> </blockquote> <p>In this first of several posts, I will go through Part I, which is titled <em>Probability</em>. Subsequent posts will tackle each part of the book in turn. This way, I can keep the posts focused and readable in about 10 minutes’ time.</p> <p>The first thing to note about this book comes before Part I even begins: <em>the visual aids are highly informative</em>. Even the graphic, Figure 1.2 on pg. 4, showing organization of the book’s parts is eye-catching and immediately makes plain the author’s intent. The book’s visuals, including graphs and tables, are laid out in such a way that complex relationships are demonstrated clearly and intuitively. The book is worth a glance for the visuals alone, to say nothing of the rest of its content. Now, let’s move on to Part I.</p> <h2 id="part-i-probability">Part I: Probability</h2> <p>Part I begins with a background on probability theory that you could get from any undergrad textbook. For example, my undergrad textbook was <a href="https://www.amazon.com/Probability-Statistics-4th-Morris-DeGroot/dp/0321500466" rel="external nofollow noopener" target="_blank"><em>Probability and Statistics</em> by DeGroot and Schervish</a>, if you are looking for a decent reference. The ideas about conditional probability, marginalization, expectation, and Bayes’ rule are presented as motivation for later chapters, but there is not anything really unexpected here. The next chapter, however, starts to get interesting in a hurry!</p> <p>In Chapter 3, Prince presents the idea of <em>conjugacy</em> of probability distributions in the context of fitting models to data. Let’s say you have training data,</p> \[X = \{x_i | 1 \le i \le I\}\] <p>and you want to find a suitable probability distribution to fit the data. Well, usually the distribution will be chosen based upon observations about the data; namely, are the data <em>continuous</em> or <em>discrete</em>. Whichever model is chosen, there will be a parameter vector, call it \(\mathbf{\theta}\), that needs to be chosen to <em>best</em> represent the data. There are two big questions/concerns that emerge at this point:</p> <ul> <li>How do we find the <em>best</em> parameters? What is meant by <em>best</em>?</li> <li>What can be said about the <em>uncertainty</em> in our chosen model parameters given the data?</li> </ul> <p>Prince addresses the second question directly on pg. 18:</p> <blockquote> <p>… when we fit probability models to data, we need to know how uncertain we are about the fit. This uncertainty is represented as a probability distribution over the parameters of the fitted model.</p> </blockquote> <p>The probability distribution over the parameters of the fitted model referred to in this quote is the <em>conjugate</em> distribution to the original model distribution used for fitting. This is a powerful idea: <em>Given data, not only can we identify and fit a particular model to these data, we can also quantify the uncertainty of our fit!</em> With well-worked examples, Prince further elaborates on this point.</p> <p>Back to the first question now: <em>How do we</em> actually_ accomplish the fit?<em>. There are three ways presented: _Maximum Likelihood Estimation, Maximum a Posteriori, and Bayesian</em>.</p> <h3 id="the-major-model-fitting-techniques">The Major Model-Fitting Techniques</h3> <h4 id="maximum-likelihood-estimation-or-mle">Maximum Likelihood Estimation, or MLE</h4> <p>Maximum Likelihood Estimation, unsurprisingly, finds the parameters that maximize the likelihood of the data observed <em>in the absence of any prior knowledge about the underlying distribution of the model parameters</em>. This last part is important, as you’ll see in the next section on the Maximum a Posteriori approach. Mathematically, MLE seeks to evaluate the following optimization objective:</p> \[\hat \theta = \text{argmax}_\theta \bigl [ Pr(X | \theta) \bigr ].\] <p>That is, we are trying to find the \(\hat \theta\) that maximizes the expression above, which assumes no prior on \(\theta\). I don’t wish for the post to get caught up in details at this point, so I’ll refer the interested reader to Chapter 4 in the book for reference. Before moving on, though, I want to briefly mention that the independence assumption of the individual data points in \(X\) is important in making the expression above solvable in closed-form for typical distributions; e.g. normal or Gaussian distributions.</p> <h4 id="maximum-a-posteriori-or-map">Maximum a Posteriori, or MAP</h4> <p>The Maximum a Posteriori approach is structurally quite similar to MLE, only the objective is now:</p> \[\hat \theta = \text{argmax}_\theta \bigl [ Pr(X | \theta) Pr(\theta) \bigr ].\] <p>Note the inclusion of the prior term \(Pr(\theta)\). If we know something about the distribution before doing the fit, we can use that prior knowledge. In the case of an uninformative prior, like \(\theta\) is drawn from a uniform distribution over its outcome space, the MAP approach <em>is exactly the same as MLE</em>!</p> <h4 id="bayesian">Bayesian</h4> <p>I think that the Bayesian approach is my favorite. Whereas the two approaches above use the data to find a <em>single</em> parameter vector $\theta$ to fit the data, the Bayesian approach does something fundamentally different. You can observe that, given any set of data and a desired model to fit it, <em>there are infinitely many parameter vectors that</em> could <em>explain the data</em>! This may require some thought, but it is particularly true in the case datasets with only a few representative samples. What the Bayesian approach seeks to find is an appropriate weighting for this continuum of parameter choices for model fitting parameters. This flexibility comes at a cost when it comes time to predict the probability of a new data point given the training data, though. This is because the probability must be computed as a weighted average <em>over all likely models</em>, where the weight comes from the fit probability \(Pr(\theta | X)\). The fit probability comes from, as you may have guessed, Bayes’ Rule:</p> \[Pr(\theta | X) = \frac{Pr(X | \theta) Pr(\theta)}{Pr(X)}.\] <p>These three approaches are cornerstones of statistical model fitting, and Prince presents them succinctly but thoroughly. Plots, such as Figures 4.6 and 4.8 on pages 35 and 37, respectively, highlight the similarities and differences between the different approaches very clearly. Specifically, the following key ideas are made plain:</p> <ul> <li>The MLE and MAP approaches can be overconfident in their predictions. Because they only seek to find a <em>single</em> parameter vector to explain the data, they may generalize poorly to data unseen during training.</li> <li>The Bayesian approach may be too cautious in its predictions. This is because predicted probabilities are an average over all <em>consistent</em> parameter vectors.</li> </ul> <h3 id="summary">Summary</h3> <p>Part I of Prince’s book presents the fundamentals needed for the remainder of the book. Each chapter is presented in 20 pages or less, which makes going through it a breeze. Oftentimes, authors will include extraneous details that derail the reader’s focus away from the core thesis, but not Prince. He only provides what is necessary for contextualizing future discussions related to the book’s core topic: computer vision. I have taken probability courses at the undergraduate and graduate levels and I wish that I would have had this book while going through them. Some things that I struggled with then would have been made plain by Prince’s insights.</p> <p>As far as the content, itself, I enjoyed writing the algorithms from pseudocode and experimenting with them given representative datasets that I created. I have made the work available on my <a href="https://github.com/jwdinius/prince-computer-vision" rel="external nofollow noopener" target="_blank">GitHub</a>, so feel free to clone and play around with the repo. I have done the experiments in Jupyter notebooks so that interested readers can edit parameters on-the-fly in their own browsers. Hopefully then they will be able to both evaluate model performance and gain insights with regards to the different models’ practical concerns.</p> <p>The next post will be more computationally-focused, as Part II is concerned with fitting of actual data and robustness to outliers. Hopefully you will read through that post as well.</p> <p>Thanks for reading!</p> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Joe Dinius, Ph.D.. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>