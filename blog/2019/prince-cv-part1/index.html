<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Joe Dinius | Review of Prince's CV Book, Part One</title>
  <meta name="description" content="Project/Blog of Joe Dinius, Ph.D. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <link rel="shortcut icon" href="https://jwdinius.github.io/assets/img/favicon.ico">

  <link rel="stylesheet" href="https://jwdinius.github.io/assets/css/main.css">
  <link rel="canonical" href="https://jwdinius.github.io/blog/2019/prince-cv-part1/">
  
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "all"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'], ['\(', '\)'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    }
  });
</script>
<!<script type="text/javascript"
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">>
<script type="text/javascript"
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

  
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Joe</strong> Dinius
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="https://jwdinius.github.io/">about</a>

        <!-- Blog -->
        <a class="page-link" href="https://jwdinius.github.io/blog/">blog</a>

        <!-- Pages -->
        
          
        
          
        
          
            <a class="page-link" href="https://jwdinius.github.io/learning/">learning</a>
          
        
          
        
          
            <a class="page-link" href="https://jwdinius.github.io/projects/">portfolio</a>
          
        
          
            <a class="page-link" href="https://jwdinius.github.io/publications/">publications</a>
          
        
          
        
          
        
          
        

        <!-- CV link -->
        <a class="page-link" href="https://jwdinius.github.io/assets/pdf/resumeDiniusTargeted.pdf">resume</a>

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Review of Prince's CV Book, Part One</h1>
    <p class="post-meta">November 9, 2019</p>
  </header>

  <article class="post-content">
    <p>Over the past few months, I have been reading through Prince’s awesome book <a href="http://www.computervisionmodels.com/"><em>Computer Vision: Models, Learning, and Inference</em></a>.  The book takes a probabilistic approach to computer vision with a remarkable level of mathematical rigor: not unapproachable, but requiring commitment to work through the details.  For me, Prince’s book strikes a much-needed balance between the academic and practical concerns for practitioners of computer vision.</p>

<blockquote>
  <p>You can find solutions to end-of-chapter exercises and algorithm implementations at my <a href="https://github.com/jwdinius/prince-computer-vision">GitHub repo</a>.  At the time of writing this post, work through Chapter 8 has been mostly completed.  I will continue adding solutions and algorithms as I review more parts, so stay tuned!</p>
</blockquote>

<p>In this first of several posts, I will go through Part I, which is titled <em>Probability</em>.  Subsequent posts will tackle each part of the book in turn.  This way, I can keep the posts focused and readable in about 10 minutes’ time.</p>

<p>The first thing to note about this book comes before Part I even begins: <em>the visual aids are highly informative</em>.  Even the graphic, Figure 1.2 on pg. 4, showing organization of the book’s parts is eye-catching and immediately makes plain the author’s intent.  The book’s visuals, including graphs and tables, are laid out in such a way that complex relationships are demonstrated clearly and intuitively.  The book is worth a glance for the visuals alone, to say nothing of the rest of its content.  Now, let’s move on to Part I.</p>

<h2 id="part-i-probability">Part I: Probability</h2>

<p>Part I begins with a background on probability theory that you could get from any undergrad textbook.  For example, my undergrad textbook was <a href="https://www.amazon.com/Probability-Statistics-4th-Morris-DeGroot/dp/0321500466"><em>Probability and Statistics</em> by DeGroot and Schervish</a>, if you are looking for a decent reference.  The ideas about conditional probability, marginalization, expectation, and Bayes’ rule are presented as motivation for later chapters, but there is not anything really unexpected here.  The next chapter, however, starts to get interesting in a hurry!</p>

<p>In Chapter 3, Prince presents the idea of <em>conjugacy</em> of probability distributions in the context of fitting models to data.  Let’s say you have training data,</p>

\[X = \{x_i | 1 \le i \le I\}\]

<p>and you want to find a suitable probability distribution to fit the data.  Well, usually the distribution will be chosen based upon observations about the data; namely, are the data <em>continuous</em> or <em>discrete</em>.  Whichever model is chosen, there will be a parameter vector, call it \(\mathbf{\theta}\), that needs to be chosen to <em>best</em> represent the data.  There are two big questions/concerns that emerge at this point:</p>

<ul>
  <li>How do we find the <em>best</em> parameters?  What is meant by <em>best</em>?</li>
  <li>What can be said about the <em>uncertainty</em> in our chosen model parameters given the data?</li>
</ul>

<p>Prince addresses the second question directly on pg. 18:</p>

<blockquote>
  <p>… when we fit probability models to data, we need to know how uncertain we are about the fit.  This uncertainty is represented as a probability distribution over the parameters of the fitted model.</p>
</blockquote>

<p>The probability distribution over the parameters of the fitted model referred to in this quote is the <em>conjugate</em> distribution to the original model distribution used for fitting.  This is a powerful idea: <em>Given data, not only can we identify and fit a particular model to these data, we can also quantify the uncertainty of our fit!</em>  With well-worked examples, Prince further elaborates on this point.</p>

<p>Back to the first question now:  <em>How do we</em> actually_ accomplish the fit?<em>.  There are three ways presented:  _Maximum Likelihood Estimation, Maximum a Posteriori, and Bayesian</em>.</p>

<h3 id="the-major-model-fitting-techniques">The Major Model-Fitting Techniques</h3>

<h4 id="maximum-likelihood-estimation-or-mle">Maximum Likelihood Estimation, or MLE</h4>

<p>Maximum Likelihood Estimation, unsurprisingly, finds the parameters that maximize the likelihood of the data observed <em>in the absence of any prior knowledge about the underlying distribution of the model parameters</em>.  This last part is important, as you’ll see in the next section on the Maximum a Posteriori approach.  Mathematically, MLE seeks to evaluate the following optimization objective:</p>

\[\hat \theta = \text{argmax}_\theta \bigl [ Pr(X | \theta) \bigr ].\]

<p>That is, we are trying to find the \(\hat \theta\) that maximizes the expression above, which assumes no prior on \(\theta\).  I don’t wish for the post to get caught up in details at this point, so I’ll refer the interested reader to Chapter 4 in the book for reference.  Before moving on, though, I want to briefly mention that the independence assumption of the individual data points in \(X\) is important in making the expression above solvable in closed-form for typical distributions; e.g. normal or Gaussian distributions.</p>

<h4 id="maximum-a-posteriori-or-map">Maximum a Posteriori, or MAP</h4>
<p>The Maximum a Posteriori approach is structurally quite similar to MLE, only the objective is now:</p>

\[\hat \theta = \text{argmax}_\theta \bigl [ Pr(X | \theta) Pr(\theta) \bigr ].\]

<p>Note the inclusion of the prior term \(Pr(\theta)\).  If we know something about the distribution before doing the fit, we can use that prior knowledge.  In the case of an uninformative prior, like \(\theta\) is drawn from a uniform distribution over its outcome space, the MAP approach <em>is exactly the same as MLE</em>!</p>

<h4 id="bayesian">Bayesian</h4>
<p>I think that the Bayesian approach is my favorite.  Whereas the two approaches above use the data to find a <em>single</em> parameter vector $\theta$ to fit the data, the Bayesian approach does something fundamentally different.  You can observe that, given any set of data and a desired model to fit it, <em>there are infinitely many parameter vectors that</em> could <em>explain the data</em>!  This may require some thought, but it is particularly true in the case datasets with only a few representative samples.  What the Bayesian approach seeks to find is an appropriate weighting for this continuum of parameter choices for model fitting parameters.  This flexibility comes at a cost when it comes time to predict the probability of a new data point given the training data, though.  This is because the probability must be computed as a weighted average <em>over all likely models</em>, where the weight comes from the fit probability \(Pr(\theta | X)\).  The fit probability comes from, as you may have guessed, Bayes’ Rule:</p>

\[Pr(\theta | X) = \frac{Pr(X | \theta) Pr(\theta)}{Pr(X)}.\]

<p>These three approaches are cornerstones of statistical model fitting, and Prince presents them succinctly but thoroughly.  Plots, such as Figures 4.6 and 4.8 on pages 35 and 37, respectively, highlight the similarities and differences between the different approaches very clearly.  Specifically, the following key ideas are made plain:</p>

<ul>
  <li>The MLE and MAP approaches can be overconfident in their predictions.  Because they only seek to find a <em>single</em> parameter vector to explain the data, they may generalize poorly to data unseen during training.</li>
  <li>The Bayesian approach may be too cautious in its predictions.  This is because predicted probabilities are an average over all <em>consistent</em> parameter vectors.</li>
</ul>

<h3 id="summary">Summary</h3>
<p>Part I of Prince’s book presents the fundamentals needed for the remainder of the book.  Each chapter is presented in 20 pages or less, which makes going through it a breeze.  Oftentimes, authors will include extraneous details that derail the reader’s focus away from the core thesis, but not Prince.  He only provides what is necessary for contextualizing future discussions related to the book’s core topic: computer vision.  I have taken probability courses at the undergraduate and graduate levels and I wish that I would have had this book while going through them.  Some things that I struggled with then would have been made plain by Prince’s insights.</p>

<p>As far as the content, itself, I enjoyed writing the algorithms from pseudocode and experimenting with them given representative datasets that I created.  I have made the work available on my <a href="https://github.com/jwdinius/prince-computer-vision">GitHub</a>, so feel free to clone and play around with the repo.  I have done the experiments in Jupyter notebooks so that interested readers can edit parameters on-the-fly in their own browsers.  Hopefully then they will be able to both evaluate model performance and gain insights with regards to the different models’ practical concerns.</p>

<p>The next post will be more computationally-focused, as Part II is concerned with fitting of actual data and robustness to outliers.  Hopefully you will read through that post as well.</p>

<p>Thanks for reading!</p>

  </article>

  
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname  = 'https-jwdinius-github-io';
      var disqus_identifier = '/blog/2019/prince-cv-part1';
      var disqus_title      = "Review of Prince's CV Book, Part One";
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2023 Joe Dinius.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://jwdinius.github.io/assets/js/common.js"></script>





<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://jwdinius.github.io/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="https://jwdinius.github.io/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXXX', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
