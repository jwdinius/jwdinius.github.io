TITLE:  Experiments with the Acrobot: Implementing a Swingup Controller and Catching a Ball
AUTHOR: Joe Dinius
DATE: today

__Summary.__
This report looks into implementing a multimodal feedback controller for the classic acrobot cite{sprong}.  The acrobot is a simple two-linked robot with two joints and control about the second joint, called the elbow (see Figure ref{fig:A1}).  The proposed controller is shown to stabilize the acrobot at the upright equilibrium, which is unstable.  The focus then shifts to finding an optimal policies for catching a ball both when the initial conditions are known and perturbed.  

## Include table of contents (latex and html; sphinx always has a toc).
## (Lines starting with ## are not propagated to the output file,
## otherwise comments lines starting with # are visible in the
## output file.)

FIGURE: [files/A1, width=200] The (in)famous acrobot. label{fig:A1}

TOC: on

!split
======= Mathematical problem =======
label{math:problem}

## Purpose: section with multi-line equation.
## idx{keyword} adds keyword to the index
## (to be place before the actual paragraph).

idx{model problem} idx{acrobot}

We address the feedback control problem for the acrobot.  For a complete discussion of the dynamics of the acrobot, see Sprong cite{sprong}.  The acrobot is a wonderful demonstrative system since it exhibits rich nonlinear behavior but is still simple enough to describe mathematically.  To stabilize about the upright equilibrium, three controllers will be used: LQR, partial feedback linearization, and energy-shaping.

The LQR feedback controller will be discussed first.  This controller's region of utility is only near where the acrobot dynamics are well-described by the linearized dynamics:

!bt
\begin{align}
\begin{pmatrix}
q \\ \dot q
\end{pmatrix} 
\approx
A
\begin{pmatrix}
q-q_0 \\ \dot q
\end{pmatrix}
+ 
B 
u, \label{lqr}
\end{align}
!et

where $q$, $q_0$, $A$, and $B$ are the two angular states, the desired angular state, A is the Jacobian matrix, and $B$ is the control Jacobian.  From this information, and through choice of appropriate symmetric positive definite matrices $Q$ and $R$, an LQR feedback controller that stabilizes the acrobot at the upright equilibrium when "sufficiently" close to the equilibrium.  Far enough away from the equilibrium, the LQR controller will not work.  This controller consists of a gain matrix, $K$, and a cost-to-go matrix, $S$.  The cost-to-go matrix determines when to switch the acrobot to using the LQR controller exclusively.  The LQR control law is:

!bt
\begin{align}
u &= -K 
\begin{pmatrix}
q - q_0 \\ \dot q
\end{pmatrix}.
\label{lqr_cntrl}
\end{align}
!et

The second controller is the partial feedback linearization controller and it is valid away from the equilibrium.  We start with the acrobot manipulator equations cite{tedrake} written in a simplified form:

!bt
\begin{align}
\ddot q &= H^{-1}(Bu-C), \label{manip}\\
B &= 
\begin{pmatrix}
0 \\ 1
\end{pmatrix}, \
C = 
\begin{pmatrix}
C_1 \\ C_2
\end{pmatrix}, \
H^{-1} = 
\begin{pmatrix}
a_1 & a_2 \\
a_2 & a_3
\end{pmatrix}.
\end{align}
!et

The goal for collocated PFL is to find a control law $u$ such that $\ddot q = y$ for some value $y$.  This law works out to be:

!bt
\begin{align}
u = \frac{1}{a_3}(y + a_2 C_1) + C_2. \label{pfl}
\end{align}
!et

The final controller is energy-shaping, and it is valid away from the upright equilibrium.  The total energy of the acrobot is the sum of the potential, $V$, and kinetic, $T$, energies:  $E = T+V$.  We seek a configuration that zeros out the change in energy; this means we've reached the equilibrium.  The resulting control law is:

!bt
\begin{align}
u = k (E_0-E) \dot q_2, \label{energy}
\end{align}
!et

where $E_0$ and $E$ are the energy at the upright state and at the current state, respectively.

The next part of the project is concerned with the generation of optimal trajectories using piecewise polynomials.  Polynomials of the form:

!bt
\begin{align}
x_i(t) = x_i(t_i) + a_1 x_i(t-t_i) + a_2 x_i^2(t-t_i) + a_3 x_i^3(t-t_i) + \ldots , \label{poly}
\end{align}
!et

where $t_i$ is the $i^{\text{th}}$ knot point in time.  The idea behind piecewise polynomial trajectory generation is to find a sequence of polynomials that agree with the dynamics of the acrobot up to some arbitrary derivative; a technique known as direct collocation cite{tedrake}.  This allows the generation of complex trajectories using relatively simple mathematics.

These methods generate an open loop optimal trajectory when the initial condition is known.  When the initial condition is perturbed, an LQR controller, which tracks error over time, must be implemented to track the optimal trajectory.  Additionally, since we are predominately concerned with the final state, where the ball is either caught or now, we need to add a final state constraint to the LQR controller.

======= Implementation =======
label{implementation}

!bwarning Note:
The implementation is built on the Drake API (see "`Drake GitHub page`":"https://github.com/RobotLocomotion/drake").  The Drake package is available freely, but some of the third-party software requires additional licensing.  The Drake repo must be build locally before you can expect any of the below code to work.
!ewarning

## Purpose: section with computer code taken from a part of
## a file. The fromto: f@t syntax copies from the regular
## expression f up to the line, but not including, the regular
## expression t.

The implementation of the full feedback controller is shown below:

!bc m
    function u = output(obj,t,~,x)
      q = x(1:2);
      qd = x(3:4);
      
      % unwrap angles q(1) to [0,2pi] and q(2) to [-pi,pi]
      q(1) = q(1) - 2*pi*floor(q(1)/(2*pi));
      q(2) = q(2) - 2*pi*floor((q(2) + pi)/(2*pi));

      %%%% put your controller here %%%%
      % You might find some of the following functions useful
      % user definitions (for first part of problem, set k2=k3=0)
      % k1 = 6 shows that energy-shaping control leads to nearly constant
      % energy when looking at position relative to lower (stable)
      % equilibrium
      k1 = 6; % energy-shaping gain
      k2 = 6; % partial feedback linearization position gain
      k3 = 6; % partial feedback linearization rate gain
      tol = 500; % cost threshold for switching to LQR feedback controller
      firstPart = 0; % set to 1 when looking at energy-shaping only controller, otherwise set 0 (false)
      
      % error vector
      if firstPart
        e  = [q;qd] - zeros(4,1); % for first part of problem (stable equilibrium)
      else
        e = [q;qd]-[pi;0;0;0]; % for second part of problem (unstable
      % equilibrium)
      end
      
      % get dynamics parameters
      [H,C,B] = obj.p.manipulatorDynamics(q,qd);
      [f,df] = obj.p.dynamics(t,[pi;0;0;0],0);
      
      % construct LQR 
      Alin = df(:,2:5);
      Blin = df(:,6);
      Q = .5*diag([1 1 1 1]);
      R = .5;
      [K,S] = lqr(Alin,Blin,Q,R);
      
      % energy-shaping piece
      com_position = obj.p.getCOM(q); % center-of-mass position
      mass = obj.p.getMass();
      gravity = obj.p.gravity;
      % Recall that the kinetic energy for a manipulator given by .5*qd'*H*qd
      T = 1/2*qd'*H*qd;
      U = -mass*gravity(3)*com_position(2);
      E = T+U;
      com_position_d = obj.p.getCOM([pi;0]);
      Ed = -mass*gravity(3)*com_position_d(2);
      ue = k1*(Ed - E)*qd(2);
      
      % partial feedback linearization piece
      Hinv = pinv(H);
      a2 = Hinv(1,2); % =H(2,1)
      a3 = Hinv(2,2);
      y  = -k2*q(2)-k3*qd(2);
      up = (y+a2*C(1))/a3 + C(2);
      
      % if the cost (e'*S*e) is smaller than some tolerance for
      % the first time, set the closer flag so that the control input
      % should be from LQR
      if (e'*S*e < tol && ~obj.closer && ~firstPart)
          obj.closer = 1;
      end
      
      % if we haven't gotten to within the linearization regime,
      % our control input should be the sum of partial feedback
      % linearization and energy-shaping inputs
      if (~obj.closer)
          u  = up+ue;
      % otherwise, if we are in a region where the linearization
      % is valid, use LQR gain applied to error vector
      else
          u = -K*e;
      end
      %%%% end of your controller %%%%
      
      % leave this line below, it limits the control input to [-20,20]
      u = max(min(u,20),-20);
      % This is the end of the function
    end
!ec

The implementation of the trajectory optimization for catching the ball is:

@@@CODE source/pset5_catch.m

When the initial condition that generated the optimal trajectory has been perturbed, and additional LQR controller needs to be implemented 

!bc m
% RUN THIS to generate your solution
megaclear

[p,xtraj,utraj,v,x0] = pset5_catch;

% if you want to display the trajectory again
%v.playback(xtraj);

% ********YOUR CODE HERE ********
% Set Q, R, and Qf for time varying LQR
% See problem statement for instructions here
xf = xtraj.eval(3);
q = xf(1:5);
qd = xf(6:10);
options = struct();
options.compute_gradients = true;
kinsol = p.doKinematics(q,qd,options);

% body index, so p.body(3) is the lower link
hand_body = 3;

% position of the "hand" on the lower link, 2.1m is the length
pos_on_hand_body = [0;-2.1];

% Calculate position of the hand in world coordinates
% the gradient, dHand_pos, is the derivative w.r.t. q
[hand_pos,dHand_pos,ddHand_pos] = p.forwardKin(kinsol,hand_body,pos_on_hand_body);

%Q = .05*eye(10);
Q = zeros(10);
R = .05;
Qf = zeros(10);
d2fdxb2 = 2;
d2fdzb2 = 2;
d2fdxbdtht1 = -2*dHand_pos(1,1); %\partial^2 f / \partial x_b \partial \theta_1
d2fdxbdtht2 = -2*dHand_pos(1,2);
d2fdzbdtht1 = -2*dHand_pos(2,1);
d2fdzbdtht2 = -2*dHand_pos(2,2);
d2fdtht12 = 2*(dHand_pos(1,1)^2+dHand_pos(2,1)^2);
d2fdtht1dtht2 = 2*(dHand_pos(1,1)*dHand_pos(1,2) + dHand_pos(2,1)*dHand_pos(2,2));
d2fdtht22 = 2*(dHand_pos(1,2)^2+dHand_pos(2,2)^2);
Qf(1:4,1:4) = [d2fdtht12 d2fdtht1dtht2 d2fdxbdtht1 d2fdzbdtht1
               d2fdtht1dtht2 d2fdtht22 d2fdxbdtht2 d2fdzbdtht2
               d2fdxbdtht1 d2fdxbdtht2 d2fdxb2 0
               d2fdzbdtht1 d2fdzbdtht2 0 d2fdzb2];
%Qf = 10*Qf;
%Qf = 60*Qf;
% *******************************
options.sqrtmethod=false;
c = p.tvlqr(xtraj,utraj,Q,R,Qf,options);
sys_cl = p.feedback(c);

%%
x0_test = x0;
x0_test(3) = x0(3) + .1;
traj_test_1 = sys_cl.simulate(xtraj.tspan,x0_test);
v.drawWrapper(traj_test_1.tspan(2),traj_test_1.eval(traj_test_1.tspan(2)));

x0_test = x0 + .02*(rand(10,1) - 1);
traj_test_2 = sys_cl.simulate(xtraj.tspan,x0_test);
v.drawWrapper(traj_test_2.tspan(2),traj_test_2.eval(traj_test_2.tspan(2)));
%playbackMovie(v,traj_test_2,'catch_perturb.avi');
!ec

======= Results =======
First, consider the case where the acrobot tries to achieve the upright equilibrium without LQR.

MOVIE: [files/swingup_cloop_noLQR.mp4,width=400] The swingup controller fails to achieve the equilibrium point. label{su_nolqr}

Now, add the LQR feedback controller:

MOVIE: [files/swingup_cloop.mp4,width=400] Adding LQR stabilizes the acrobot. label{su_lqr}

The result of the open loop optimization for catching a ball under perfect knowledge of initial condition:

MOVIE: [files/first_catch.mp4,width=400] Open loop optimal trajectory catches ball. label{ol_catch}

When the initial condition is perturbed, but the original open loop optimization is used, a trajectory can result which does not catch the ball:


MOVIE: [files/nocatch_perturb.mp4,width=400] Perturbed optimal trajectory fails to catch the ball. label{cl_nocatch}


To fix this, an LQR feedback controller with final cost constraint can be implemented that will penalize the acrobot for states far from where the ball should be caught.

MOVIE: [files/catch_perturb.mp4,width=400] Closed loop optimal trajectory catches ball when initial condition has been perturbed. label{cl_catch}

!bsummary
 o A multimodal controller employing (1) LQR, (2) partial feedback linearization, and (3) energy-shaping controllers was implemented to stabilize the acrobot about its upright equilibrium.
 o The LQR controller is required for stabilization once the acrobot gets close to the equilibrium point.  Close can be defined by the cost-to-go matrix $S$.
 o Open loop trajectory optimization was used to catch a ball with the acrobot from an arbitrary initial condition.
 o When the initial condition used to generate the optimal trajectory is perturbed, the acrobot no longer catches the ball.  An LQR controller with final state constraint can be implemented which tracks the original optimal trajectory and catches the ball.
!esummary

======= References =======

BIBFILE:  files/papers.pub