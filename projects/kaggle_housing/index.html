<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Housing Price Prediction | Joe Dinius, Ph.D.</title> <meta name="author" content="Joe Dinius, Ph.D."> <meta name="description" content="Kaggle Competition"> <meta name="keywords" content="robotics, autonomy, math, optimization, controltheory, planning, computervision"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?df44dccb15554b4d2d173cb203488555"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?53ca2be8c4ec0d533ef79017d1a2d734"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jwdinius.github.io/projects/kaggle_housing/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Joe </span>Dinius, Ph.D.</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">git</a> </li> <li class="nav-item "> <a class="nav-link" href="/learning/">learning</a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/resumeDiniusTargeted.pdf">resume</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Housing Price Prediction</h1> <p class="post-description">Kaggle Competition</p> </header> <article> <h2 id="abstract">Abstract</h2> <p>A number of interesting data exploration, visualization, and engineering techniques are employed to build a predictive regressor for housing prices based upon a rich feature set. Among these techniques are heatmaps, box-plots, feature engineering, and gradient boosted trees. An initial naive regressor is constructed (using random forests), which is extended by introducing gradient boosted trees. Iterative parameter tuning using grid and random search techniques achieves large increases in model predictive capability.</p> <h2 id="i--definition">I. Definition</h2> <h3 id="project-overview">Project Overview</h3> <p>The website <a href="https://www.kaggle.com" rel="external nofollow noopener" target="_blank">Kaggle</a> recently hosted a competition that requires implementation of regression techniques like those used in the <a href="https://www.udacity.com/wiki/ud675/project" rel="external nofollow noopener" target="_blank">Boston housing prediction project</a>. Sberbank, the Russian bank, along with Kaggle, is hosting a competition to predict Russian housing prices based on a dataset rich in features. According to the <a href="https://www.kaggle.com/c/sberbank-russian-housing-market" rel="external nofollow noopener" target="_blank">project introduction page</a>, the Russian housing market is relatively stable when compared to the more volatile Russian economy, as a whole. Having an accurate predictor would help to assure both investors and prospective property owners that they are making wise choices in how they invest in Russian real estate despite other potentially worrying economic indicators.</p> <h3 id="problem-statement">Problem Statement</h3> <p>Kaggle has provided a full dataset broken into training and test datasets. This dataset provides features from which a regression model will be used to predict housing prices based on the values of these features. A standard regression technique such as the one used for the Boston housing prediction project will most likely not perform well on the larger, more nuanced feature set that will be used for this project. A more flexible learner is needed. The package <a href="http://xgboost.readthedocs.io/en/latest" rel="external nofollow noopener" target="_blank">XGBoost</a> is a fully integrated, flexible framework for performing gradient boosting regression using multiple simple learners, such as decision trees. The combination of simple learners into a more complex single learner has been very successful (see <a href="">this</a>). The reason XGBoost is chosen as the starting point is its <a href="https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions" rel="external nofollow noopener" target="_blank">success</a> in previous machine learning competitions.</p> <p>The solution to this project has two parts: (1) tuning a regressor to minimize the prediction error of housing prices on validation data extracted from the training set provided and (2) predicting prices on the Kaggle-provided test set using regressors developed during the tuning process. The prediction data on the test set will be submitted to Kaggle for internal computation of a public score. The public score is used to rank solutions and will be used as a sanity check on the algorithm’s performance. The desired public score is as close to 0 as possible; lower scores are better than higher ones.</p> <h3 id="metrics">Metrics</h3> <p>The primary metric here, as in most regression problems, is error on a test set with known output. The error metric used for optimization is the negative of the <a href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="external nofollow noopener" target="_blank">mean squared error</a>; this provides a strong estimate of how well matched predictions are to actual observations. Since the error is averaged over all elements of a dataset (training or validation), this provides a quantitative assessment of model quality. The negative is chosen on account of the <code class="language-plaintext highlighter-rouge">sklearn</code> protocol where the negative is used in a maximization scheme; that is, minimizing error is equivalent to maximizing the negative of that same error.</p> <p>Another metric will be used as well. In cross-validation efforts, the model’s predictive power will be evaluated using the R2 score, also known as the <a href="http://stattrek.com/statistics/dictionary.aspx?definition=coefficient_of_determination" rel="external nofollow noopener" target="_blank">coefficient of determination</a>. A score of <code class="language-plaintext highlighter-rouge">1</code> means that the model perfectly predicts the output for each given input, while a score of <code class="language-plaintext highlighter-rouge">-1</code> means that the model is completely unable to predict output based on the given input. The goal is an R2 score that is positive and as close to 1 as possible.</p> <h2 id="ii-analysis">II. Analysis</h2> <p>As a starting point, <a href="https://www.kaggle.com/captcalculator/a-very-extensive-sberbank-exploratory-analysis" rel="external nofollow noopener" target="_blank">this</a> was used for reference.</p> <h3 id="data-exploration">Data Exploration</h3> <p>The dataset provided is captured in three csv files: <a href="./data/train.csv">train.csv</a>, <a href="./data/test.csv">test.csv</a>, and <a href="./data/macro.csv">macro.csv</a>. The columnar data of the csv files are described in <a href="./data/data_dictionary.txt">data_dictionary.txt</a>; please see this file for description of the individual features. The columns in train.csv and test.csv represent data about the properties themselves, along with timestamps; data such as square footage (size), number of rooms, and build year. Extrinsic properties, such as proximity to entertainment and cultural attractions, is also included in these csv files. The column <code class="language-plaintext highlighter-rouge">price_doc</code> denotes the sale prices of the properties, and is the target variable for the regression analysis. Data within macro.csv, as the name would indicate, presents broad macroeconomic indicators along with timestamps. These data include gross domestic product, currency exchange rates, and mortgage rates. Now, load the data using the <code class="language-plaintext highlighter-rouge">pandas</code> package:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="p">.</span><span class="nf">color_palette</span><span class="p">()</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">pd</span><span class="p">.</span><span class="n">options</span><span class="p">.</span><span class="n">mode</span><span class="p">.</span><span class="n">chained_assignment</span> <span class="o">=</span> <span class="bp">None</span>  <span class="c1"># default='warn'
</span><span class="n">pd</span><span class="p">.</span><span class="nf">set_option</span><span class="p">(</span><span class="sh">'</span><span class="s">display.max_columns</span><span class="sh">'</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">"</span><span class="s">./data/train.csv</span><span class="sh">"</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">timestamp</span><span class="sh">'</span><span class="p">])</span>
<span class="n">train_df_copy</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

<span class="n">train_df</span><span class="p">.</span><span class="nf">drop</span><span class="p">([</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">"</span><span class="s">./data/test.csv</span><span class="sh">"</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">timestamp</span><span class="sh">'</span><span class="p">])</span>
<span class="n">test_df</span><span class="p">.</span><span class="nf">drop</span><span class="p">([</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">macro_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">"</span><span class="s">./data/macro.csv</span><span class="sh">"</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">timestamp</span><span class="sh">'</span><span class="p">])</span>

<span class="n">all_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span><span class="p">])</span>
<span class="n">all_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">merge_ordered</span><span class="p">(</span><span class="n">test_df</span><span class="p">,</span> <span class="n">macro_df</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="sh">'</span><span class="s">timestamp</span><span class="sh">'</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="sh">'</span><span class="s">left</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Remove timestamp column (may overfit the model in train)
</span><span class="n">all_df</span><span class="p">.</span><span class="nf">drop</span><span class="p">([</span><span class="sh">'</span><span class="s">timestamp</span><span class="sh">'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p>Before going any further, some sense of how the sales price, <code class="language-plaintext highlighter-rouge">price_doc</code>, should be determined. This can be accomplished with a simple histogram:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ax</span> <span class="o">=</span> <span class="n">train_df_copy</span><span class="p">[</span><span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">].</span><span class="nf">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">price</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">$N$</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">output_images/price_doc_hist.png</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/assets/img/capstone_1_files/capstone_1_16_0.png" alt="png"></p> <p>The data appears to be positive-definite and may well be lognormal distributed. In regression analyses, when some variables appear to be lognormal, <a href="https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/#four" rel="external nofollow noopener" target="_blank">taking the log can reduce variation caused by outliers in the original distribution</a>. The histogram is replotted after adding 1 (to avoid divergence of the logarithm function) and then taking the natural logarithm:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_df_copy</span><span class="p">[</span><span class="sh">'</span><span class="s">price_doc_log</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log1p</span><span class="p">(</span><span class="n">train_df_copy</span><span class="p">[</span><span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">])</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">train_df_copy</span><span class="p">[</span><span class="sh">'</span><span class="s">price_doc_log</span><span class="sh">'</span><span class="p">].</span><span class="nf">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">$\log($ price $+1)$</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">$N$</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">output_images/price_doc_log_hist.png</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/assets/img/capstone_1_files/capstone_1_18_0.png" alt="png"></p> <p>The resulting histogram above seems to indicate a mostly normal distribution; there is apparent symmetry about a central value between 15 and 16. Going forward, the transformed price variable will be used as the new target variable for the regression analysis. This should enable the model to be more robust to variation caused by extrema in the price data.</p> <p>Next, explore the top-level statistics of the intrinsic properties of the real estate:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span> <span class="sh">"</span><span class="s">full_sq (metric): mean=%.2f,</span><span class="sh">"</span> <span class="o">%</span> <span class="n">all_df</span><span class="p">[</span><span class="sh">'</span><span class="s">full_sq</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">(),</span> <span class="sh">"</span><span class="s">std=%.2f,</span><span class="sh">"</span> <span class="o">%</span> <span class="n">all_df</span><span class="p">[</span><span class="sh">'</span><span class="s">full_sq</span><span class="sh">'</span><span class="p">].</span><span class="nf">std</span><span class="p">(),</span> <span class="sh">"</span><span class="s">median=%.2f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">all_df</span><span class="p">[</span><span class="sh">'</span><span class="s">full_sq</span><span class="sh">'</span><span class="p">].</span><span class="nf">median</span><span class="p">()</span>
<span class="k">print</span> <span class="sh">"</span><span class="s">life_sq (metric): mean=%.2f,</span><span class="sh">"</span> <span class="o">%</span> <span class="n">all_df</span><span class="p">[</span><span class="sh">'</span><span class="s">life_sq</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">(),</span> <span class="sh">"</span><span class="s">std=%.2f,</span><span class="sh">"</span> <span class="o">%</span> <span class="n">all_df</span><span class="p">[</span><span class="sh">'</span><span class="s">life_sq</span><span class="sh">'</span><span class="p">].</span><span class="nf">std</span><span class="p">(),</span> <span class="sh">"</span><span class="s">median=%.2f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">all_df</span><span class="p">[</span><span class="sh">'</span><span class="s">life_sq</span><span class="sh">'</span><span class="p">].</span><span class="nf">median</span><span class="p">()</span>
<span class="k">print</span> <span class="sh">"</span><span class="s">kitch_sq (metric): mean=%.2f,</span><span class="sh">"</span> <span class="o">%</span> <span class="n">all_df</span><span class="p">[</span><span class="sh">'</span><span class="s">kitch_sq</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">(),</span> <span class="sh">"</span><span class="s">std=%.2f,</span><span class="sh">"</span> <span class="o">%</span> <span class="n">all_df</span><span class="p">[</span><span class="sh">'</span><span class="s">kitch_sq</span><span class="sh">'</span><span class="p">].</span><span class="nf">std</span><span class="p">(),</span> <span class="sh">"</span><span class="s">median=%.2f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">all_df</span><span class="p">[</span><span class="sh">'</span><span class="s">kitch_sq</span><span class="sh">'</span><span class="p">].</span><span class="nf">median</span><span class="p">()</span>
<span class="k">print</span> <span class="sh">"</span><span class="s">num_room: mean=%.2f,</span><span class="sh">"</span> <span class="o">%</span> <span class="n">all_df</span><span class="p">[</span><span class="sh">'</span><span class="s">num_room</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">(),</span> <span class="sh">"</span><span class="s">std=%.2f,</span><span class="sh">"</span> <span class="o">%</span> <span class="n">all_df</span><span class="p">[</span><span class="sh">'</span><span class="s">num_room</span><span class="sh">'</span><span class="p">].</span><span class="nf">std</span><span class="p">(),</span> <span class="sh">"</span><span class="s">median=%.2f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">all_df</span><span class="p">[</span><span class="sh">'</span><span class="s">num_room</span><span class="sh">'</span><span class="p">].</span><span class="nf">median</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>full_sq (metric): mean=53.70, std=20.10, median=50.41
life_sq (metric): mean=32.66, std=22.74, median=30.40
kitch_sq (metric): mean=6.94, std=25.58, median=7.00
num_room: mean=1.88, std=0.84, median=2.00
</code></pre></div></div> <p>The above variables show close connection between the mean and median values. What is surprising is the large variation in kitchen size. In fact, there is larger variance in this feature than in the full property size!</p> <p>One might like to get a better idea of how some of these data are distributed. One such data trend that would of interest would be price versus the year the property was built.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="sh">'</span><span class="s">90</span><span class="sh">'</span><span class="p">)</span>
<span class="c1"># ignore properties which are too old or are yet to be built
</span><span class="n">ind</span> <span class="o">=</span> <span class="n">train_df_copy</span><span class="p">[(</span><span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">build_year</span><span class="sh">'</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">1691</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">train_df_copy</span><span class="p">[</span><span class="sh">'</span><span class="s">build_year</span><span class="sh">'</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">2018</span><span class="p">)].</span><span class="n">index</span>
<span class="n">by_df</span> <span class="o">=</span> <span class="n">train_df_copy</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">ind</span><span class="p">).</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">build_year</span><span class="sh">'</span><span class="p">])</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">countplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">by_df</span><span class="p">[</span><span class="sh">'</span><span class="s">build_year</span><span class="sh">'</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sh">'</span><span class="s">Number of Properties Built in a Given Year</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">output_images/properties_year.png</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/assets/img/capstone_1_files/capstone_1_22_0.png" alt="png"></p> <p>From the plot above, it is clear that a majority of properties sold have been built more recently (<code class="language-plaintext highlighter-rouge">build_year &gt; 2013</code>). However, it is interesting to note the large amount of construction in the late 1950’s and early 1960’s, and the subsequent dip in housing production culminating in a minimum in 1991, near when the Soviet Union collapsed. This is purely speculation at this point, and doesn’t really hold much weight in the analysis that follows. However, it is not inconceivable that such a trend would positively correlate with macroeconomic indicators, such as GDP.</p> <p>As kind of an aside, check the mean sale price as a function of build year:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">by_price</span> <span class="o">=</span> <span class="n">by_df</span><span class="p">.</span><span class="nf">groupby</span><span class="p">(</span><span class="sh">'</span><span class="s">build_year</span><span class="sh">'</span><span class="p">)[[</span><span class="sh">'</span><span class="s">build_year</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">]].</span><span class="nf">mean</span><span class="p">()</span>
<span class="c1"># fit a third order polynomial to the data, along with error bars
</span><span class="n">sns</span><span class="p">.</span><span class="nf">regplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="sh">"</span><span class="s">build_year</span><span class="sh">"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">"</span><span class="s">price_doc</span><span class="sh">"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">by_price</span><span class="p">,</span> <span class="n">scatter</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">truncate</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># plot the actual data
</span><span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">by_price</span><span class="p">[</span><span class="sh">'</span><span class="s">build_year</span><span class="sh">'</span><span class="p">],</span> <span class="n">by_price</span><span class="p">[</span><span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sh">'</span><span class="s">Mean price by year of build</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">output_images/mean_price_year.png</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/assets/img/capstone_1_files/capstone_1_24_0.png" alt="png"></p> <p>The above plot demonstrates a more easily predictable trend for properties built more recently (<code class="language-plaintext highlighter-rouge">build_year &gt; ~1945</code>). The inherent accuracy of such predictions can be seen from the tight variance bounds around the simple regression plotted alongside the raw data. The reduction in variability could be a result of more uniformity in housing projects during the Communist years, or for some other reasons. Again, this is merely an observation and doesn’t impact the analysis that follows.</p> <p>Another nice plot would be something that would show price as a function of the condition of the property. A box-plot would work, and the <code class="language-plaintext highlighter-rouge">state</code> variable encodes the conditions of the properties:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">stripplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="sh">"</span><span class="s">state</span><span class="sh">"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">"</span><span class="s">price_doc</span><span class="sh">"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">train_df_copy</span><span class="p">,</span> <span class="n">jitter</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="p">.</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">.8</span><span class="sh">"</span><span class="p">);</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="sh">"</span><span class="s">state</span><span class="sh">"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">"</span><span class="s">price_doc</span><span class="sh">"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">train_df_copy</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sh">'</span><span class="s">Distribution of Home Price by Property Condition</span><span class="sh">'</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="sh">'</span><span class="s">state</span><span class="sh">'</span><span class="p">,</span> 
       <span class="n">ylabel</span><span class="o">=</span><span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">output_images/state_box_plot.png</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/assets/img/capstone_1_files/capstone_1_27_0.png" alt="png"></p> <p>The above plot demonstrates a few interesting points. The first is a data quality issue; namely, what does <code class="language-plaintext highlighter-rouge">state = 33</code> mean? After looking into the dataset itself and at forums on the Kaggle competition page, this seems to be an invalid state. Given the frequency of occurrence and the easy-to-do data entry error of entering multiple of the same number in the same data cell, it seems the entry was made in error and the appropriate state in these instances should be <code class="language-plaintext highlighter-rouge">state = 3</code>. The original dataset can now be updated in light of this new information and the data can be replotted.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># state should be discrete valued between 1 and 4. There is a 33 in it that is cleary a data entry error
# replace it with 3.
</span><span class="n">all_df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">all_df</span><span class="p">[</span><span class="sh">'</span><span class="s">state</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">33</span><span class="p">,</span> <span class="sh">'</span><span class="s">state</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">train_df_copy</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">train_df_copy</span><span class="p">[</span><span class="sh">'</span><span class="s">state</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">33</span><span class="p">,</span> <span class="sh">'</span><span class="s">state</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># replot the box-plot with the update
</span><span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">stripplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="sh">"</span><span class="s">state</span><span class="sh">"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">"</span><span class="s">price_doc</span><span class="sh">"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">train_df_copy</span><span class="p">,</span> <span class="n">jitter</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="p">.</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">.8</span><span class="sh">"</span><span class="p">);</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="sh">"</span><span class="s">state</span><span class="sh">"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">"</span><span class="s">price_doc</span><span class="sh">"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">train_df_copy</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sh">'</span><span class="s">Distribution of Home Price by Property Condition</span><span class="sh">'</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="sh">'</span><span class="s">state</span><span class="sh">'</span><span class="p">,</span> 
       <span class="n">ylabel</span><span class="o">=</span><span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">output_images/state_box_plot_fixed.png</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/assets/img/capstone_1_files/capstone_1_29_0.png" alt="png"></p> <p>The trends observed above tend to make sense. Price increases with increasing condition, or quality, of the property. Also, properties with the lowest and highest conditions occur less frequently than those properties in just so-so condition. Properties in the highest condition tend to have higher variability in price. This could be as a result of high quality properties having variability in size, number of rooms, and other intrinsic properties. There may be other trends as well; namely, extrinsic properties (e.g. proximity to shopping, grocery stores, schools, etc…) may have an impact on price.</p> <p>Before moving on with the data exploration, another irregularity is present in the dataset. This was highlighted in the Kaggle forums for the competition and is pretty simple to fix.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># build_year has an erronus value 20052009.  Just pick the middle of 2005, 2009
</span><span class="n">train_df_copy</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">train_df_copy</span><span class="p">[</span><span class="sh">'</span><span class="s">build_year</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">20052009</span><span class="p">,</span> <span class="sh">'</span><span class="s">build_year</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2007</span>
</code></pre></div></div> <p>Through inspection of the raw data, it’s clear that much data is missing (indicated by <code class="language-plaintext highlighter-rouge">NA</code> in the cells). The most frequently missing data, along with the frequency missing, is plotted below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">all_na</span> <span class="o">=</span> <span class="p">(</span><span class="n">all_df</span><span class="p">.</span><span class="nf">isnull</span><span class="p">().</span><span class="nf">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">all_df</span><span class="p">))</span> 
<span class="n">all_na</span> <span class="o">=</span> <span class="n">all_na</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">all_na</span><span class="p">[</span><span class="n">all_na</span> <span class="o">==</span> <span class="mi">0</span><span class="p">].</span><span class="n">index</span><span class="p">).</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">train_na</span> <span class="o">=</span> <span class="p">(</span><span class="n">train_df_copy</span><span class="p">.</span><span class="nf">isnull</span><span class="p">().</span><span class="nf">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_df_copy</span><span class="p">))</span> 
<span class="n">train_na</span> <span class="o">=</span> <span class="n">train_na</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">train_na</span><span class="p">[</span><span class="n">train_na</span> <span class="o">==</span> <span class="mi">0</span><span class="p">].</span><span class="n">index</span><span class="p">).</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="sh">'</span><span class="s">90</span><span class="sh">'</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">train_na</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">train_na</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sh">'</span><span class="s">Frequency of Missing Data by Feature</span><span class="sh">'</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="sh">'</span><span class="s">ratio missing</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">output_images/missing_frequency.png</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/assets/img/capstone_1_files/capstone_1_34_0.png" alt="png"></p> <p>(It should be mentioned here that “raion” indicates an area around the property, kind of like a region would be in English-speaking nations.)</p> <p>For some of the data, it is not particularly surprising that values are missing. Something like the number of hospital beds in a region may be a difficult number to come by in a real estate search. However, it is worth noting that a number of common property features are missing with surprisingly large frequency. Factors such as <code class="language-plaintext highlighter-rouge">state</code>, the condition of the property should definitely be recorded as part of any real estate transaction. If it were simply missing because there is no property state (i.e. the property is a vacant lot), then this should be encoded as a separate class within the data.</p> <h3 id="exploratory-visualization">Exploratory Visualization</h3> <p>Before asking any more questions of the data, all of it needs to be compiled together: both the macro- and microeconomic data.</p> <p>After compilation, the first thing to consider is the importance of individual features. Arguably, the best starting point is how price relates to intrinsic features of properties. Heatmaps show how correlated one variable is to others and are great indicators of interrelatedness within data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">internal_chars</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">full_sq</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">life_sq</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">floor</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">max_floor</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">build_year</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">num_room</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">kitch_sq</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">state</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">]</span>
<span class="n">corrmat</span> <span class="o">=</span> <span class="n">train_df_copy</span><span class="p">[</span><span class="n">internal_chars</span><span class="p">].</span><span class="nf">corr</span><span class="p">()</span>

<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="sh">'</span><span class="s">90</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Heatmap- Intrinsic Features</span><span class="sh">'</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">corrmat</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">output_images/intrinsic_heatmap.png</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/assets/img/capstone_1_files/capstone_1_38_0.png" alt="png"></p> <p>The heatmap above shows interrelatedness between intrinsic data of properties.</p> <p>The biggest correlations to price are seen in <code class="language-plaintext highlighter-rouge">full_sq</code> and <code class="language-plaintext highlighter-rouge">num_room</code>. This makes sense; increasing square footage and number of rooms should increase the sale price. Also, not surprisingly, the state and floor of the property (in the case of apartments) are also positively correlated with sale price.</p> <p>Before moving on, it’s a good idea to get a sense of where most properties are being sold.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">sa_vc</span> <span class="o">=</span> <span class="n">train_df_copy</span><span class="p">[</span><span class="sh">'</span><span class="s">sub_area</span><span class="sh">'</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">()</span>
<span class="n">sa_vc</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">'</span><span class="s">sub_area</span><span class="sh">'</span><span class="p">:</span><span class="n">sa_vc</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="sh">'</span><span class="s">count</span><span class="sh">'</span><span class="p">:</span> <span class="n">sa_vc</span><span class="p">.</span><span class="n">values</span><span class="p">})</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="p">.</span><span class="nf">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="sh">"</span><span class="s">count</span><span class="sh">"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">"</span><span class="s">sub_area</span><span class="sh">"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">sa_vc</span><span class="p">,</span> <span class="n">orient</span><span class="o">=</span><span class="sh">"</span><span class="s">h</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sh">'</span><span class="s">Transactions by District</span><span class="sh">'</span><span class="p">)</span>
<span class="n">f</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">output_images/transactions_district.png</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/assets/img/capstone_1_files/capstone_1_41_0.png" alt="png"></p> <p>The distribution of the data above shows a nearly exponential decay, so that an exponentially greater number of properties are being sold in some provinces (areas) than in others. This is reasonable; it seems plausible that areas with higher populations would see more real estate transactions.</p> <p>It’s time to move on. In the heatmap presented previously, trends between intrinsic properties were made clear. These data showed a somewhat predictable trend: more square footage tended to mean a higher sales price. Less clear, however, is the correlation between extrinsic properties and price. Again, a heatmap will be illuminating here. Look at some of the more broad features, features pertaining to local population.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Deal with categorical values on train set
</span><span class="n">numeric_df</span> <span class="o">=</span> <span class="n">train_df_copy</span><span class="p">.</span><span class="nf">select_dtypes</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">object</span><span class="sh">'</span><span class="p">])</span>
<span class="n">obj_df</span> <span class="o">=</span> <span class="n">train_df_copy</span><span class="p">.</span><span class="nf">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">object</span><span class="sh">'</span><span class="p">]).</span><span class="nf">copy</span><span class="p">()</span>

<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">obj_df</span><span class="p">:</span>
    <span class="n">obj_df</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">factorize</span><span class="p">(</span><span class="n">obj_df</span><span class="p">[</span><span class="n">c</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">values_df_copy</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">numeric_df</span><span class="p">,</span> <span class="n">obj_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Deal with categorical values on full set
</span><span class="n">numeric_df</span> <span class="o">=</span> <span class="n">all_df</span><span class="p">.</span><span class="nf">select_dtypes</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">object</span><span class="sh">'</span><span class="p">])</span>
<span class="n">obj_df</span> <span class="o">=</span> <span class="n">all_df</span><span class="p">.</span><span class="nf">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">object</span><span class="sh">'</span><span class="p">]).</span><span class="nf">copy</span><span class="p">()</span>

<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">obj_df</span><span class="p">:</span>
    <span class="n">obj_df</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">factorize</span><span class="p">(</span><span class="n">obj_df</span><span class="p">[</span><span class="n">c</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">values_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">numeric_df</span><span class="p">,</span> <span class="n">obj_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">demo_vars</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">area_m</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">raion_popul</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">full_all</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">male_f</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">female_f</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">young_all</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">young_female</span><span class="sh">'</span><span class="p">,</span> 
             <span class="sh">'</span><span class="s">work_all</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">work_male</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">work_female</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">sub_area</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">]</span>
<span class="n">corrmat</span> <span class="o">=</span> <span class="n">values_df_copy</span><span class="p">[</span><span class="n">demo_vars</span><span class="p">].</span><span class="nf">corr</span><span class="p">()</span>

<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="sh">'</span><span class="s">90</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Heatmap- Extrinsic Features</span><span class="sh">'</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">corrmat</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">output_images/extrinsic_heatmap.png</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/assets/img/capstone_1_files/capstone_1_45_0.png" alt="png"></p> <p>The variables shown above are mostly concerned with aspects of the regions themselves. These data include region population, along with a breakdown on working age (<code class="language-plaintext highlighter-rouge">work_*</code>) and youth (<code class="language-plaintext highlighter-rouge">young_*</code>) populations. Not surprisingly, there is moderate correlation between the region name and the area of that region. This isn’t, strictly-speaking, important to the analysis; it’s just an observation. Also, the population characteristics are highly correlated to one another; that is, the working age male population of an area (<code class="language-plaintext highlighter-rouge">work_male</code>) is highly correlated with the working age female population (<code class="language-plaintext highlighter-rouge">work_female</code>) of that same area.</p> <p>There doesn’t appear to be a strong correlation to any of these variables with the sales price. Maybe other extrinsic features will show more correlation. There are so many of these features, so maybe break them down into categories: school, entertainment, and infrastructure.</p> <p>Consider school characteristics first:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">school_chars</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">children_preschool</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">preschool_quota</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">preschool_education_centers_raion</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">children_school</span><span class="sh">'</span><span class="p">,</span> 
                <span class="sh">'</span><span class="s">school_quota</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">school_education_centers_raion</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">school_education_centers_top_20_raion</span><span class="sh">'</span><span class="p">,</span> 
                <span class="sh">'</span><span class="s">university_top_20_raion</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">additional_education_raion</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">additional_education_km</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">university_km</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">]</span>
<span class="n">corrmat</span> <span class="o">=</span> <span class="n">values_df_copy</span><span class="p">[</span><span class="n">school_chars</span><span class="p">].</span><span class="nf">corr</span><span class="p">()</span>

<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="sh">'</span><span class="s">90</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Heatmap- Extrinsic Features, School</span><span class="sh">'</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">corrmat</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">output_images/school_heatmap.png</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/assets/img/capstone_1_files/capstone_1_47_0.png" alt="png"></p> <p>The population of school-age children shows a moderate correlation with sales price, as expected. The degree of correlation is less than expected given naive assumptions about proximity to schools being a prime factor in real estate in the United States. Of note, as well, is the negative correlation of sales prices with distance to universities. It makes sense that, the farther one is from a university, the lower the sales price. This trend exists in the US as well, since real estate close to universities is very desirable, and fetches a larger sales price typically.</p> <p>Next, consider entertainment features:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cult_chars</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">sport_objects_raion</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">culture_objects_top_25_raion</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">shopping_centers_raion</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">park_km</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">fitness_km</span><span class="sh">'</span><span class="p">,</span> 
                <span class="sh">'</span><span class="s">swim_pool_km</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ice_rink_km</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">stadium_km</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">basketball_km</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">shopping_centers_km</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">theater_km</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">museum_km</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">exhibition_km</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">catering_km</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">]</span>
<span class="n">corrmat</span> <span class="o">=</span> <span class="n">values_df_copy</span><span class="p">[</span><span class="n">cult_chars</span><span class="p">].</span><span class="nf">corr</span><span class="p">()</span>

<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="sh">'</span><span class="s">90</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Heatmap- Extrinsic Features, Entertainment</span><span class="sh">'</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">corrmat</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">output_images/entertainment_heatmap.png</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/assets/img/capstone_1_files/capstone_1_49_0.png" alt="png"></p> <p>The data above again makes sense in the aggregate: the farther a property is from recreational attractions, like shopping centers or museums, the lower the sales price. Put another way, real estate near cultural and recreational attractions is more desirable and is typically more valuable.</p> <p>Finally, infrastructure features are considered:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inf_features</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">nuclear_reactor_km</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">thermal_power_plant_km</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">power_transmission_line_km</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">incineration_km</span><span class="sh">'</span><span class="p">,</span>
                <span class="sh">'</span><span class="s">water_treatment_km</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">railroad_station_walk_min</span><span class="sh">'</span><span class="p">,</span> 
                <span class="sh">'</span><span class="s">railroad_station_avto_min</span><span class="sh">'</span><span class="p">,</span> 
                <span class="sh">'</span><span class="s">public_transport_station_min_walk</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">water_km</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">mkad_km</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ttk_km</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">sadovoe_km</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">bulvar_ring_km</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">]</span>
<span class="n">corrmat</span> <span class="o">=</span> <span class="n">values_df_copy</span><span class="p">[</span><span class="n">inf_features</span><span class="p">].</span><span class="nf">corr</span><span class="p">()</span>

<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="sh">'</span><span class="s">90</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Heatmap- Extrinsic Features, Infrastructure</span><span class="sh">'</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">corrmat</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">output_images/infrastructure_heatmap.png</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/assets/img/capstone_1_files/capstone_1_51_0.png" alt="png"></p> <p>The data above is surprising. Naively, one would assume that being closer to power generation stations would be undesirable and would show positive correlation with price. However, the data indicates otherwise. Other data, like correlation of price with distance to public transportation, makes sense in the context provided.</p> <p>Next, consider correlation between price and macroeconomic indicators:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">values_df</span><span class="p">[</span><span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_df_copy</span><span class="p">[</span><span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">]</span>
<span class="n">macro_features</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">cpi</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">gdp_annual</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">gdp_annual_growth</span><span class="sh">'</span><span class="p">,</span> 
                  <span class="sh">'</span><span class="s">deposits_value</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">deposits_growth</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">deposits_rate</span><span class="sh">'</span><span class="p">,</span>
                  <span class="sh">'</span><span class="s">mortgage_value</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">mortgage_growth</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">mortgage_rate</span><span class="sh">'</span><span class="p">,</span>
                  <span class="sh">'</span><span class="s">unemployment</span><span class="sh">'</span><span class="p">,</span>
                  <span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">]</span>
<span class="n">corrmat</span> <span class="o">=</span> <span class="n">values_df</span><span class="p">[</span><span class="n">macro_features</span><span class="p">].</span><span class="nf">corr</span><span class="p">()</span>

<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="sh">'</span><span class="s">90</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Heatmap- Macroeconomic Features</span><span class="sh">'</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">corrmat</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">output_images/macro_heatmap.png</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/assets/img/capstone_1_files/capstone_1_53_0.png" alt="png"></p> <p>This data is pretty interesting. As expected, intrinsic and extrinsic properties of the real estate are much more correlated to sales price than broader macroeconomic features. This is exactly as one would expect. It is surprising, though, that mortgage properties aren’t more positively corelated with price. It would seem that lower mortgage rates might entice more buyers, and with more buyers, one might expect higher prices. However, the data shows almost no correlation to price. It is also interesting that prices aren’t more negatively correlated with unempoyment. Again, naively, it would seem that higher unemplyment would mean lower prices on real estate transactions.</p> <h3 id="algorithms-and-techniques">Algorithms and Techniques</h3> <p>As mentioned previously, <code class="language-plaintext highlighter-rouge">XGBoost</code>, along with the <code class="language-plaintext highlighter-rouge">sklearn</code> wrapper <code class="language-plaintext highlighter-rouge">XGBRegressor</code>, will be used for performing the regression analysis. This package was chosen because of its success in machine learning competitions of various scales. The use of boosted trees will allow for a model that requires minimal feature engineering (see <a href="https://www.kaggle.com/c/allstate-claims-severity/discussion/24500" rel="external nofollow noopener" target="_blank">this</a>); all that needs to be done is hyperparameter tuning. The package <code class="language-plaintext highlighter-rouge">sklearn</code> has two very nice optimization frameworks for estimator tuning: <code class="language-plaintext highlighter-rouge">RandomizedSearchCV</code> and <code class="language-plaintext highlighter-rouge">GridSearchCV</code>. <code class="language-plaintext highlighter-rouge">RandomizedSearchCV</code> allows for sampling the parameter space by random draws from probability distributions. The choice of distribution depends upon the parameter being tuned, however the uniform distribution is a great choice; this is because it allows for sampling of a spatial structure where each point has equal probability. The equal probability is desirable since, a priori, there are minimal assumptions made about model performance as a function of the hyperparameters.</p> <p>As with all regression techniques, a model is developed here which minimizes error of a predicted target variable versus a known target variable from a training set comprised of feature vectors. How well the model performs is a function of several things; chief among them being choice of hyperparameters and structure of the input feature data. Both of these things will be discussed subsequently in the Methodology section.</p> <h3 id="benchmark">Benchmark</h3> <p>No real established regression benchmark exists for this dataset. However, a simple process can be used to create a benchmark that can be built on through hyperparameter tuning. <a href="http://www-bcf.usc.edu/~gareth/ISL" rel="external nofollow noopener" target="_blank">Random forest regressors</a> provide a simple out-of-the box regressor that can be improved upon with more advanced regression techniques (e.g. XGBoost). Random forests provide comparable performance to boosting methods (see <a href="http://fastml.com/what-is-better-gradient-boosted-trees-or-random-forest/" rel="external nofollow noopener" target="_blank">this</a>) without having to perform much tuning to optimize. The <code class="language-plaintext highlighter-rouge">sklearn.ensemble</code> package has <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html" rel="external nofollow noopener" target="_blank">RandomForestRegressor</a>. The performance using this regressor is:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># load packages
</span><span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">Imputer</span>
<span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># reload the data (a lot of processing happened above, clean it up a bit)
</span><span class="n">df_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">"</span><span class="s">./data/train.csv</span><span class="sh">"</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">timestamp</span><span class="sh">'</span><span class="p">])</span>
<span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">"</span><span class="s">./data/test.csv</span><span class="sh">"</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">timestamp</span><span class="sh">'</span><span class="p">])</span>
<span class="n">df_macro</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">"</span><span class="s">./data/macro.csv</span><span class="sh">"</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">timestamp</span><span class="sh">'</span><span class="p">])</span>

<span class="c1"># ylog will be log(1+y), as suggested by https://github.com/dmlc/xgboost/issues/446#issuecomment-135555130
</span><span class="n">ylog_train_all</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log1p</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
<span class="n">id_test</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">]</span>

<span class="n">df_train</span><span class="p">.</span><span class="nf">drop</span><span class="p">([</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df_test</span><span class="p">.</span><span class="nf">drop</span><span class="p">([</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Build df_all = (df_train+df_test).join(df_macro)
</span><span class="n">num_train</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">df_train</span><span class="p">)</span>
<span class="n">df_all</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span><span class="p">])</span>
<span class="n">df_all</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">merge_ordered</span><span class="p">(</span><span class="n">df_all</span><span class="p">,</span> <span class="n">df_macro</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="sh">'</span><span class="s">timestamp</span><span class="sh">'</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="sh">'</span><span class="s">left</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Remove timestamp column (may overfit the model in train)
</span><span class="n">df_all</span><span class="p">.</span><span class="nf">drop</span><span class="p">([</span><span class="sh">'</span><span class="s">timestamp</span><span class="sh">'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Deal with categorical values
</span><span class="n">df_numeric</span> <span class="o">=</span> <span class="n">df_all</span><span class="p">.</span><span class="nf">select_dtypes</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">object</span><span class="sh">'</span><span class="p">])</span>
<span class="n">df_obj</span> <span class="o">=</span> <span class="n">df_all</span><span class="p">.</span><span class="nf">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">object</span><span class="sh">'</span><span class="p">]).</span><span class="nf">copy</span><span class="p">()</span>

<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">df_obj</span><span class="p">:</span>
    <span class="n">df_obj</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">factorize</span><span class="p">(</span><span class="n">df_obj</span><span class="p">[</span><span class="n">c</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">df_values</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">df_numeric</span><span class="p">,</span> <span class="n">df_obj</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">X_all</span> <span class="o">=</span> <span class="n">df_values</span><span class="p">.</span><span class="n">values</span>

<span class="c1"># do feature scaling (try it out at least) comment out the next two lines if not needed/wanted
# Fit a per-column scaler
</span><span class="n">X_scaler</span> <span class="o">=</span> <span class="nc">Imputer</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">missing_values</span><span class="o">=</span><span class="sh">'</span><span class="s">NaN</span><span class="sh">'</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_all</span><span class="p">)</span>
<span class="c1"># Apply the scaler to X
</span><span class="n">scaled_X</span> <span class="o">=</span> <span class="n">X_scaler</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_all</span><span class="p">)</span>

<span class="c1"># uncomment to avoid feature scaling
#scaled_X = X_all
</span>
<span class="c1"># Create a validation set, with 30% of data
</span><span class="n">num_train</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">df_train</span><span class="p">)</span>
<span class="n">X_train_all</span> <span class="o">=</span> <span class="n">scaled_X</span><span class="p">[:</span><span class="n">num_train</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">ylog_train</span><span class="p">,</span> <span class="n">ylog_val</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X_train_all</span><span class="p">,</span> <span class="n">ylog_train_all</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span> 

<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaled_X</span><span class="p">[</span><span class="n">num_train</span><span class="p">:]</span>

<span class="n">df_columns</span> <span class="o">=</span> <span class="n">df_values</span><span class="p">.</span><span class="n">columns</span><span class="p">[:</span><span class="nf">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>

<span class="k">print</span> <span class="sh">'</span><span class="s">X shape is</span><span class="sh">'</span><span class="p">,</span> <span class="n">scaled_X</span><span class="p">.</span><span class="n">shape</span> 
<span class="k">print</span> <span class="sh">'</span><span class="s">X_train shape is</span><span class="sh">'</span><span class="p">,</span> <span class="n">X_train</span><span class="p">.</span><span class="n">shape</span> 
<span class="k">print</span> <span class="sh">'</span><span class="s">y_train shape is</span><span class="sh">'</span><span class="p">,</span> <span class="n">ylog_train</span><span class="p">.</span><span class="n">shape</span>
<span class="k">print</span> <span class="sh">'</span><span class="s">X_val shape is</span><span class="sh">'</span><span class="p">,</span> <span class="n">X_val</span><span class="p">.</span><span class="n">shape</span>
<span class="k">print</span> <span class="sh">'</span><span class="s">y_val shape is</span><span class="sh">'</span><span class="p">,</span> <span class="n">ylog_val</span><span class="p">.</span><span class="n">shape</span>
<span class="k">print</span> <span class="sh">'</span><span class="s">X_test shape is</span><span class="sh">'</span><span class="p">,</span> <span class="n">X_test</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X shape is (38133, 388)
X_train shape is (21329, 388)
y_train shape is (21329,)
X_val shape is (9142, 388)
y_val shape is (9142,)
X_test shape is (7662, 388)
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">estimator</span> <span class="o">=</span> <span class="nc">RandomForestRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">estimator</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">ylog_train</span><span class="p">)</span>
<span class="k">print</span> <span class="sh">"</span><span class="s">Score on the validation set (~0.30 of training set) = %.5f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">estimator</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">ylog_val</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  6.1min finished


Score on the validation set (~0.30 of training set) = 0.39009


[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.4s finished
</code></pre></div></div> <p>Part of this project is to see how well different models perform with respect to <a href="https://www.kaggle.com/c/sberbank-russian-housing-market/leaderboard" rel="external nofollow noopener" target="_blank">Kaggle’s leaderboard score</a>. To create the submission csv file to be scored, execute the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># predict the log(1+p)
</span><span class="n">ylog_pred</span> <span class="o">=</span> <span class="n">estimator</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># convert to price
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">ylog_pred</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

<span class="c1"># create pandas dataframe
</span><span class="n">df_sub</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">:</span> <span class="n">id_test</span><span class="p">,</span> <span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">:</span> <span class="n">y_pred</span><span class="p">})</span>
<span class="c1"># write csv file
</span><span class="n">df_sub</span><span class="p">.</span><span class="nf">to_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">sub_rfr.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.3s finished
</code></pre></div></div> <p>The public leaderboard score for this submission was 0.41168, which is significantly worse than the current best score of 0.30728. But, as a first shot with no tuning, this isn’t terrible.</p> <p>Another preliminary plot can be made now that this estimator has been created: feature importance can be estimated using the attribute <code class="language-plaintext highlighter-rouge">estimator.feature_importances_</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">importances</span> <span class="o">=</span> <span class="n">estimator</span><span class="p">.</span><span class="n">feature_importances_</span>
<span class="n">sd</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">([</span><span class="n">estimator</span><span class="p">.</span><span class="n">feature_importances_</span> <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="n">estimator</span><span class="p">.</span><span class="n">estimators_</span><span class="p">],</span>
             <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># only take the top 25 so the plot isn't a mess
</span><span class="n">ind</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[:</span><span class="mi">25</span><span class="p">]</span>

<span class="c1"># Print the feature ranking
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Feature ranking:</span><span class="sh">"</span><span class="p">)</span>

<span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">ind</span><span class="p">)):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">%d. %s (%f)</span><span class="sh">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">f</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">df_columns</span><span class="p">[</span><span class="n">f</span><span class="p">],</span> <span class="n">importances</span><span class="p">[</span><span class="n">indices</span><span class="p">[</span><span class="n">f</span><span class="p">]]))</span>

<span class="c1"># Plot the feature importances of the forest
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Feature importances</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">ind</span><span class="p">)),</span> <span class="n">importances</span><span class="p">[</span><span class="n">ind</span><span class="p">],</span>
       <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">sd</span><span class="p">[</span><span class="n">ind</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="sh">"</span><span class="s">center</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">ind</span><span class="p">)),</span> <span class="n">df_columns</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="sh">'</span><span class="s">90</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">ind</span><span class="p">)])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">output_images/importance.png</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Feature ranking:
1. full_sq (0.244700)
2. life_sq (0.027183)
3. floor (0.021952)
4. max_floor (0.019117)
5. material (0.015946)
6. build_year (0.009856)
7. num_room (0.008115)
8. kitch_sq (0.007545)
9. state (0.007489)
10. area_m (0.007484)
11. raion_popul (0.007328)
12. green_zone_part (0.007096)
13. indust_part (0.006897)
14. children_preschool (0.006834)
15. preschool_quota (0.006734)
16. preschool_education_centers_raion (0.006707)
17. children_school (0.006621)
18. school_quota (0.006155)
19. school_education_centers_raion (0.006115)
20. school_education_centers_top_20_raion (0.006093)
21. hospital_beds_raion (0.006048)
22. healthcare_centers_raion (0.005918)
23. university_top_20_raion (0.005895)
24. sport_objects_raion (0.005874)
25. additional_education_raion (0.005752)
</code></pre></div></div> <p><img src="/assets/img/capstone_1_files/capstone_1_65_1.png" alt="png"></p> <p>This makes sense, since one would expect that the most important features, with regard to sales price, would be the size of the property and recreational/entertainment features nearby (cafes, sports, etc…).</p> <h2 id="iii-methodology">III. Methodology</h2> <p>As a starting point, <a href="https://www.kaggle.com/bguberfain/naive-xgb-lb-0-317" rel="external nofollow noopener" target="_blank">this</a> was used as a reference.</p> <h3 id="data-preprocessing-and-algorithm-implementation">Data Preprocessing and Algorithm Implementation</h3> <p>There are a few different features that are not modeled as of yet, but could be important. Such features include the month, day-of-week, and the year of the transaction:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_all</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span><span class="p">])</span>
<span class="n">df_all</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">merge_ordered</span><span class="p">(</span><span class="n">df_all</span><span class="p">,</span> <span class="n">df_macro</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="sh">'</span><span class="s">timestamp</span><span class="sh">'</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="sh">'</span><span class="s">left</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Add month, day-of-week, and year
</span><span class="n">df_all</span><span class="p">[</span><span class="sh">'</span><span class="s">month</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_all</span><span class="p">.</span><span class="n">timestamp</span><span class="p">.</span><span class="n">dt</span><span class="p">.</span><span class="n">month</span>
<span class="n">df_all</span><span class="p">[</span><span class="sh">'</span><span class="s">dow</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_all</span><span class="p">.</span><span class="n">timestamp</span><span class="p">.</span><span class="n">dt</span><span class="p">.</span><span class="n">dayofweek</span>
<span class="n">df_all</span><span class="p">[</span><span class="sh">'</span><span class="s">year</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_all</span><span class="p">.</span><span class="n">timestamp</span><span class="p">.</span><span class="n">dt</span><span class="p">.</span><span class="n">year</span>

<span class="c1"># Remove timestamp column (may overfit the model in train)
</span><span class="n">df_all</span><span class="p">.</span><span class="nf">drop</span><span class="p">([</span><span class="sh">'</span><span class="s">timestamp</span><span class="sh">'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p>It would seem reasonable that some months would be better for purchasing property than others; real estate may be less desirable in winte months, for example. Also, the year of the transaction may be significant if there are significant economic or social trends that were important that year (e.g. the fall of the Soviet Union in the early-1990’s).</p> <p>The ratio of room space to total space may also be interesting. For instance, someone might not want a tiny kitchen in an otherwise large home. Also, apartments on the higher floors will most likely fetch higher prices than those on lower floors. Therefore, one should look at the ratio of the floor to the maximum number of floors in an apartment building. A small “fudge-factor” is added to the division in the <code class="language-plaintext highlighter-rouge">rel_floor</code> value computation to avoid divide-by-zero issues.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Other feature engineering
</span><span class="n">df_all</span><span class="p">[</span><span class="sh">'</span><span class="s">rel_floor</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_all</span><span class="p">[</span><span class="sh">'</span><span class="s">floor</span><span class="sh">'</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">df_all</span><span class="p">[</span><span class="sh">'</span><span class="s">max_floor</span><span class="sh">'</span><span class="p">]</span><span class="o">+</span><span class="mf">1.e-6</span><span class="p">)).</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">df_all</span><span class="p">[</span><span class="sh">'</span><span class="s">rel_kitch_sq</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_all</span><span class="p">[</span><span class="sh">'</span><span class="s">kitch_sq</span><span class="sh">'</span><span class="p">]</span> <span class="o">/</span> <span class="n">df_all</span><span class="p">[</span><span class="sh">'</span><span class="s">full_sq</span><span class="sh">'</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">df_all</span><span class="p">[</span><span class="sh">'</span><span class="s">rel_life_sq</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_all</span><span class="p">[</span><span class="sh">'</span><span class="s">life_sq</span><span class="sh">'</span><span class="p">]</span> <span class="o">/</span> <span class="n">df_all</span><span class="p">[</span><span class="sh">'</span><span class="s">full_sq</span><span class="sh">'</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
</code></pre></div></div> <p>Now, address with the categorical values again in the new derived dataset. Also, deal with <code class="language-plaintext highlighter-rouge">NaN</code>’s and scale the data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Deal with categorical values
</span><span class="n">df_numeric</span> <span class="o">=</span> <span class="n">df_all</span><span class="p">.</span><span class="nf">select_dtypes</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">object</span><span class="sh">'</span><span class="p">])</span>
<span class="n">df_obj</span> <span class="o">=</span> <span class="n">df_all</span><span class="p">.</span><span class="nf">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">object</span><span class="sh">'</span><span class="p">]).</span><span class="nf">copy</span><span class="p">()</span>

<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">df_obj</span><span class="p">:</span>
    <span class="n">df_obj</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">factorize</span><span class="p">(</span><span class="n">df_obj</span><span class="p">[</span><span class="n">c</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">df_values</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">df_numeric</span><span class="p">,</span> <span class="n">df_obj</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Convert to numpy values
</span><span class="n">X_all</span> <span class="o">=</span> <span class="n">df_values</span><span class="p">.</span><span class="n">values</span>

<span class="c1"># do feature scaling (try it out at least) comment out the next two lines if not needed/wanted
# Fit a per-column scaler
</span><span class="n">X_scaler</span> <span class="o">=</span> <span class="nc">Imputer</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">missing_values</span><span class="o">=</span><span class="sh">'</span><span class="s">NaN</span><span class="sh">'</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_all</span><span class="p">)</span>
<span class="c1"># Apply the scaler to X
</span><span class="n">scaled_X</span> <span class="o">=</span> <span class="n">X_scaler</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_all</span><span class="p">)</span>
</code></pre></div></div> <p>It should be clear from the heatmaps that the data is very <a href="https://www.kaggle.com/robertoruiz/dealing-with-multicollinearity" rel="external nofollow noopener" target="_blank">collinear</a>. This would seem to indicate that there might be a gain in model performance if the feature set was reduced to a combination of linearly independent pseudofeatures. This could be accomplished via <a href="http://setosa.io/ev/principal-component-analysis/" rel="external nofollow noopener" target="_blank">principal components analysis</a>. First, a baseline needs to be established to see if PCA might help.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">import</span> <span class="n">xgboost</span> <span class="k">as</span> <span class="n">xgb</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/Users/jdinius/miniconda2/envs/ml_capstone/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train_all</span> <span class="o">=</span> <span class="n">scaled_X</span><span class="p">[:</span><span class="n">num_train</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">ylog_train</span><span class="p">,</span> <span class="n">ylog_val</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X_train_all</span><span class="p">,</span> <span class="n">ylog_train_all</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span> 

<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaled_X</span><span class="p">[</span><span class="n">num_train</span><span class="p">:]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">estimator</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="nc">XGBRegressor</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">silent</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">estimator</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">ylog_train</span><span class="p">)</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">estimator</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">ylog_val</span><span class="p">)</span>
<span class="k">print</span> <span class="sh">"</span><span class="s">Score on the validation set (~0.30 of training set) = %.7f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">score</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Score on the validation set (~0.30 of training set) = 0.4097120
</code></pre></div></div> <p>Create a submission for this simple effort with no PCA.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># predict the log(1+p)
</span><span class="n">ylog_pred</span> <span class="o">=</span> <span class="n">estimator</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># convert to price
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">ylog_pred</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

<span class="c1"># create pandas dataframe
</span><span class="n">df_sub</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">:</span> <span class="n">id_test</span><span class="p">,</span> <span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">:</span> <span class="n">y_pred</span><span class="p">})</span>
<span class="c1"># write csv file
</span><span class="n">df_sub</span><span class="p">.</span><span class="nf">to_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">sub_xgb_base.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <p>The public leaderboard score is 0.39276, which is a little better than the random forests’ score. But, maybe performance might be better with PCA. First, define a helper function to aid in the analysis.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipeline</span><span class="p">(</span><span class="n">X_input</span><span class="o">=</span><span class="n">scaled_X</span><span class="p">,</span> <span class="n">ylog_input</span><span class="o">=</span><span class="n">ylog_train_all</span><span class="p">,</span> <span class="n">n_comp</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_train</span><span class="o">=</span><span class="n">num_train</span><span class="p">,</span> <span class="n">whiten</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_comp</span><span class="p">,</span><span class="n">whiten</span><span class="o">=</span><span class="n">whiten</span><span class="p">)</span>
    <span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X_input</span><span class="p">)</span>
    <span class="n">X_train_all</span> <span class="o">=</span> <span class="n">X_pca</span><span class="p">[:</span><span class="n">num_train</span><span class="p">]</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">ylog_train</span><span class="p">,</span> <span class="n">ylog_val</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X_train_all</span><span class="p">,</span> <span class="n">ylog_input</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span> 
    
    <span class="n">estimator</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="nc">XGBRegressor</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">silent</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">estimator</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">ylog_train</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">estimator</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">ylog_val</span><span class="p">)</span>
    <span class="k">print</span> <span class="sh">"</span><span class="s">Score on the validation set (~0.30 of training set) = %.7f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">score</span>
    <span class="k">return</span> <span class="n">score</span>
</code></pre></div></div> <p>This function will take in data to split into training and validation sets. To ensure proper comparison, common seeds for both the split and regressors will be used. This will ensure that algorithm performance improvements will truly be due to choice of principal components and not due to random variation. The R2 score is used as determinator. Now, create a simple routine to select and evaluate sensitivity of XGBoost to principal component selection.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">randint</span> <span class="k">as</span> <span class="n">sp_randint</span>
<span class="n">low</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># floor on pc's allowed
</span><span class="n">high</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">scaled_X</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">n_trials</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">old_score</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span>
<span class="n">rvs</span> <span class="o">=</span> <span class="n">sp_randint</span><span class="p">.</span><span class="nf">rvs</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_trials</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">rnd</span> <span class="ow">in</span> <span class="n">rvs</span><span class="p">:</span>
    <span class="k">print</span> <span class="n">rnd</span><span class="p">,</span> <span class="sh">"</span><span class="s"> components...</span><span class="sh">"</span>
    <span class="n">score</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span><span class="n">n_comp</span><span class="o">=</span><span class="n">rnd</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="n">old_score</span><span class="p">:</span>
        <span class="n">old_score</span> <span class="o">=</span> <span class="n">score</span>
        <span class="n">n_pca</span> <span class="o">=</span> <span class="n">rnd</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>182  components...
Score on the validation set (~0.30 of training set) = 0.3749615
57  components...
Score on the validation set (~0.30 of training set) = 0.1795186
127  components...
Score on the validation set (~0.30 of training set) = 0.3753255
202  components...
Score on the validation set (~0.30 of training set) = 0.3751032
333  components...
Score on the validation set (~0.30 of training set) = 0.3751862
261  components...
Score on the validation set (~0.30 of training set) = 0.3755494
205  components...
Score on the validation set (~0.30 of training set) = 0.3741650
369  components...
Score on the validation set (~0.30 of training set) = 0.3742034
19  components...
Score on the validation set (~0.30 of training set) = 0.1530518
221  components...
Score on the validation set (~0.30 of training set) = 0.3723765
</code></pre></div></div> <p>This didn’t improve model performance. In fact, performance was degraded. It’s also clear that there seems to be nearly indistinguishable scores once the number of principal components exceeds some threshold (about 127 in this case). There is another PCA option that can be utilized to try that might improve performance: whitening. Whitening the data is a way to further remove correlation from feature vectors resulting from a principal components transformation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">old_score</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span>
<span class="n">rvs</span> <span class="o">=</span> <span class="n">sp_randint</span><span class="p">.</span><span class="nf">rvs</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_trials</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">rnd</span> <span class="ow">in</span> <span class="n">rvs</span><span class="p">:</span>
    <span class="k">print</span> <span class="n">rnd</span><span class="p">,</span> <span class="sh">"</span><span class="s"> components...</span><span class="sh">"</span>
    <span class="n">score</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span><span class="n">n_comp</span><span class="o">=</span><span class="n">rnd</span><span class="p">,</span> <span class="n">whiten</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="n">old_score</span><span class="p">:</span>
        <span class="n">old_score</span> <span class="o">=</span> <span class="n">score</span>
        <span class="n">n_pca</span> <span class="o">=</span> <span class="n">rnd</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>182  components...
Score on the validation set (~0.30 of training set) = 0.3734841
57  components...
Score on the validation set (~0.30 of training set) = 0.1819242
127  components...
Score on the validation set (~0.30 of training set) = 0.3749593
202  components...
Score on the validation set (~0.30 of training set) = 0.3730136
333  components...
Score on the validation set (~0.30 of training set) = 0.3751862
261  components...
Score on the validation set (~0.30 of training set) = 0.3740765
205  components...
Score on the validation set (~0.30 of training set) = 0.3726132
369  components...
Score on the validation set (~0.30 of training set) = 0.3742034
19  components...
Score on the validation set (~0.30 of training set) = 0.1572138
221  components...
Score on the validation set (~0.30 of training set) = 0.3722941
</code></pre></div></div> <p>These results are indistinguishable from the non-whitening results. Therefore it is concluded that it is not worth using principal components; stick with the dataset as-is and focus on tuning the hyperparameters for XGBoost.</p> <h3 id="refinement">Refinement</h3> <p>Following <a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" rel="external nofollow noopener" target="_blank">this procedure</a>, the tuning process is as follows:</p> <table> <thead> <tr> <th>Step #:</th> <th>Process</th> </tr> </thead> <tbody> <tr> <td>Step 1:</td> <td>Tune <code class="language-plaintext highlighter-rouge">max_depth</code> and <code class="language-plaintext highlighter-rouge">min_child_weight</code> </td> </tr> <tr> <td>Step 2:</td> <td>Tune <code class="language-plaintext highlighter-rouge">gamma</code> </td> </tr> <tr> <td>Step 3:</td> <td>Tune <code class="language-plaintext highlighter-rouge">subsample</code> and <code class="language-plaintext highlighter-rouge">colsample_bytree</code> </td> </tr> <tr> <td>Step 4:</td> <td>Tune regularization parameters</td> </tr> <tr> <td>Step 5:</td> <td>Tune <code class="language-plaintext highlighter-rouge">learning_rate</code> </td> </tr> <tr> <td>Step 6:</td> <td>Tune <code class="language-plaintext highlighter-rouge">n_estimators</code> </td> </tr> </tbody> </table> <p>The functions <code class="language-plaintext highlighter-rouge">RandomizedSearchCV</code> and <code class="language-plaintext highlighter-rouge">GridSearchCV</code> functions from <code class="language-plaintext highlighter-rouge">sklearn.model_selection</code> are used for the optimization. The resource referenced also provides the range of values to use for the tuning. See <a href="http://xgboost.readthedocs.io/en/latest/parameter.html" rel="external nofollow noopener" target="_blank">this</a> for more details on hyperparameter descriptions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="n">time</span> <span class="kn">import</span> <span class="n">time</span>
</code></pre></div></div> <p>Define a helper function to report the top scores from the tuning steps.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Utility function to report best scores
</span><span class="k">def</span> <span class="nf">report</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">n_top</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_top</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">candidates</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">flatnonzero</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">rank_test_score</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="n">i</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">candidate</span> <span class="ow">in</span> <span class="n">candidates</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Model with rank: {0}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Mean validation score: {0:.5f} (std: {1:.5f})</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span>
                  <span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">mean_test_score</span><span class="sh">'</span><span class="p">][</span><span class="n">candidate</span><span class="p">],</span>
                  <span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">std_test_score</span><span class="sh">'</span><span class="p">][</span><span class="n">candidate</span><span class="p">]))</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Parameters: {0}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">][</span><span class="n">candidate</span><span class="p">]))</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">""</span><span class="p">)</span>

    <span class="k">return</span>
</code></pre></div></div> <h4 id="step-1-tune-max_depth-and-min_child_weight">Step 1: Tune <code class="language-plaintext highlighter-rouge">max_depth</code> and <code class="language-plaintext highlighter-rouge">min_child_weight</code> </h4> <p>The parameters <code class="language-plaintext highlighter-rouge">max_depth</code> and <code class="language-plaintext highlighter-rouge">min_child_weight</code> are the number of layers of decision trees used and the minimum sum of instance weight in a tree’s child, respectively.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fixed_params_rd1</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">n_estimators</span><span class="sh">'</span><span class="p">:</span> <span class="mi">500</span>
<span class="p">}</span>

<span class="c1"># specify parameters and distributions to sample from
</span><span class="n">cv_params_rd1</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">max_depth</span><span class="sh">"</span><span class="p">:</span> <span class="nf">sp_randint</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">11</span><span class="p">),</span>
                 <span class="sh">"</span><span class="s">min_child_weight</span><span class="sh">"</span><span class="p">:</span> <span class="nf">sp_randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">)}</span>

<span class="c1"># randomized search
</span><span class="n">n_iter_search</span> <span class="o">=</span> <span class="mi">25</span>
<span class="n">optimized_XGB</span> <span class="o">=</span> <span class="nc">RandomizedSearchCV</span><span class="p">(</span><span class="n">xgb</span><span class="p">.</span><span class="nc">XGBRegressor</span><span class="p">(</span><span class="o">**</span><span class="n">fixed_params_rd1</span><span class="p">),</span>
                               <span class="n">cv_params_rd1</span><span class="p">,</span>
                               <span class="n">scoring</span> <span class="o">=</span> <span class="sh">'</span><span class="s">neg_mean_squared_error</span><span class="sh">'</span><span class="p">,</span>
                               <span class="n">cv</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
                               <span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter_search</span><span class="p">,</span>
                               <span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
                               <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>


<span class="n">start</span> <span class="o">=</span> <span class="nf">time</span><span class="p">()</span>

<span class="n">optimized_XGB</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">ylog_train</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">RandomizedSearchCV took %.2f seconds for %d candidates</span><span class="sh">"</span>
      <span class="sh">"</span><span class="s"> parameter settings.</span><span class="sh">"</span> <span class="o">%</span> <span class="p">((</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">),</span> <span class="n">n_iter_search</span><span class="p">))</span>
<span class="nf">report</span><span class="p">(</span><span class="n">optimized_XGB</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Fitting 5 folds for each of 25 candidates, totalling 125 fits


[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 38.8min
[Parallel(n_jobs=4)]: Done 125 out of 125 | elapsed: 106.2min finished


RandomizedSearchCV took 6455.26 seconds for 25 candidates parameter settings.
Model with rank: 1
Mean validation score: -0.21743 (std: 0.00763)
Parameters: {'max_depth': 3, 'min_child_weight': 5}

Model with rank: 2
Mean validation score: -0.21993 (std: 0.00843)
Parameters: {'max_depth': 4, 'min_child_weight': 2}

Model with rank: 3
Mean validation score: -0.22030 (std: 0.00792)
Parameters: {'max_depth': 4, 'min_child_weight': 3}
</code></pre></div></div> <p>It is a little unclear what the optimal value for <code class="language-plaintext highlighter-rouge">min_child_weight</code> is, since the value reported for the optimal estimator is at the ceiling of the values tested. Therefore, test values greater than this and see if performance improves.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">estimator</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="nc">XGBRegressor</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">silent</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">estimator</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">ylog_train</span><span class="p">)</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">estimator</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">ylog_val</span><span class="p">)</span>
<span class="k">print</span> <span class="sh">"</span><span class="s">Score on the validation set (~0.30 of training set) = %.5f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">score</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Score on the validation set (~0.30 of training set) = 0.41664
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">estimator</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="nc">XGBRegressor</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">silent</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">estimator</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">ylog_train</span><span class="p">)</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">estimator</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">ylog_val</span><span class="p">)</span>
<span class="k">print</span> <span class="sh">"</span><span class="s">Score on the validation set (~0.30 of training set) = %.5f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">score</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Score on the validation set (~0.30 of training set) = 0.41860
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">estimator</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="nc">XGBRegressor</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">silent</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">estimator</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">ylog_train</span><span class="p">)</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">estimator</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">ylog_val</span><span class="p">)</span>
<span class="k">print</span> <span class="sh">"</span><span class="s">Score on the validation set (~0.30 of training set) = %.5f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">score</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Score on the validation set (~0.30 of training set) = 0.41821
</code></pre></div></div> <p>Performance improved a bit with <code class="language-plaintext highlighter-rouge">min_child_weight</code> increased to 6, so this value will be used going forward. The csv for Kaggle performance is now created below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># predict the log(1+p)
</span><span class="n">ylog_pred</span> <span class="o">=</span> <span class="n">estimator</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># convert to price
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">ylog_pred</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

<span class="c1"># create pandas dataframe
</span><span class="n">df_sub</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">:</span> <span class="n">id_test</span><span class="p">,</span> <span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">:</span> <span class="n">y_pred</span><span class="p">})</span>
<span class="c1"># write csv file
</span><span class="n">df_sub</span><span class="p">.</span><span class="nf">to_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">sub_xgb_rd1.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <p>The public score is 0.34795, which is a significant improvement over the previous score of 0.39276.</p> <h4 id="step-2--tune-gamma">Step 2: Tune <code class="language-plaintext highlighter-rouge">gamma</code> </h4> <p>The hyperparameter <code class="language-plaintext highlighter-rouge">gamma</code> is the minimum loss reduction required to create a further partition. This can also be thought of as information gain required from a certain split in order to make that split.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">uniform</span> <span class="k">as</span> <span class="n">sp_uniform</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fixed_params_rd2</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">n_estimators</span><span class="sh">'</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">max_depth</span><span class="sh">"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">min_child_weight</span><span class="sh">"</span><span class="p">:</span> <span class="mi">6</span>
<span class="p">}</span>

<span class="c1"># specify parameters and distributions to sample from
</span><span class="n">rvs</span> <span class="o">=</span> <span class="nf">sp_uniform</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">cv_params_rd2</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">gamma</span><span class="sh">"</span><span class="p">:</span> <span class="n">rvs</span><span class="p">}</span>

<span class="c1"># randomized search
</span><span class="n">n_iter_search</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">optimized_XGB</span> <span class="o">=</span> <span class="nc">RandomizedSearchCV</span><span class="p">(</span><span class="n">xgb</span><span class="p">.</span><span class="nc">XGBRegressor</span><span class="p">(</span><span class="o">**</span><span class="n">fixed_params_rd2</span><span class="p">),</span>
                               <span class="n">cv_params_rd2</span><span class="p">,</span>
                               <span class="n">scoring</span> <span class="o">=</span> <span class="sh">'</span><span class="s">neg_mean_squared_error</span><span class="sh">'</span><span class="p">,</span>
                               <span class="n">cv</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
                               <span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter_search</span><span class="p">,</span>
                               <span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
                               <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>


<span class="n">start</span> <span class="o">=</span> <span class="nf">time</span><span class="p">()</span>

<span class="n">optimized_XGB</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">ylog_train</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">RandomizedSearchCV took %.2f seconds for %d candidates</span><span class="sh">"</span>
      <span class="sh">"</span><span class="s"> parameter settings.</span><span class="sh">"</span> <span class="o">%</span> <span class="p">((</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">),</span> <span class="n">n_iter_search</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Fitting 5 folds for each of 15 candidates, totalling 75 fits


[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 15.8min
[Parallel(n_jobs=4)]: Done  75 out of  75 | elapsed: 27.2min finished


RandomizedSearchCV took 1713.08 seconds for 15 candidates parameter settings.
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">report</span><span class="p">(</span><span class="n">optimized_XGB</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">)</span>

<span class="c1"># predict the log(1+p)
</span><span class="n">ylog_pred</span> <span class="o">=</span> <span class="n">optimized_XGB</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># convert to price
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">ylog_pred</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

<span class="c1"># create pandas dataframe
</span><span class="n">df_sub</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">:</span> <span class="n">id_test</span><span class="p">,</span> <span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">:</span> <span class="n">y_pred</span><span class="p">})</span>
<span class="c1"># write csv file
</span><span class="n">df_sub</span><span class="p">.</span><span class="nf">to_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">sub_xgb_rd2.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">optimized_XGB</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">ylog_val</span><span class="p">)</span>
<span class="k">print</span> <span class="sh">"</span><span class="s">Score on the validation set (~0.30 of training set) = %.5f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">score</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model with rank: 1
Mean validation score: -0.21635 (std: 0.00712)
Parameters: {'gamma': 0.12181820933103936}

Model with rank: 2
Mean validation score: -0.21650 (std: 0.00729)
Parameters: {'gamma': 0.20868217554374732}

Model with rank: 3
Mean validation score: -0.21650 (std: 0.00700)
Parameters: {'gamma': 0.1456312910685548}

Score on the validation set (~0.30 of training set) = 0.41756
</code></pre></div></div> <p>The public score for this step is 0.34814. Both the public and R2 scores are not significantly different than those from the previous step. Both are still an improvement over the random forest baseline.</p> <h4 id="step-3--tune-subsample-and-colsample_bytree">Step 3: Tune <code class="language-plaintext highlighter-rouge">subsample</code> and <code class="language-plaintext highlighter-rouge">colsample_bytree</code> </h4> <p>The hyperparameters <code class="language-plaintext highlighter-rouge">subsample</code> and <code class="language-plaintext highlighter-rouge">colsample_bytree</code> are the subsample ratio of training data and the subsample ratio of columns when constructing a new tree, respectively. The first parameter is to prevent overfitting to the training data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fixed_params_rd3</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">n_estimators</span><span class="sh">'</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">max_depth</span><span class="sh">"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">min_child_weight</span><span class="sh">"</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span> 
    <span class="sh">'</span><span class="s">gamma</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.122</span>
<span class="p">}</span>

<span class="c1"># specify parameters and distributions to sample from
</span><span class="n">rvs_1</span> <span class="o">=</span> <span class="nf">sp_uniform</span><span class="p">(</span><span class="mf">0.6</span><span class="p">,</span><span class="mf">0.4</span><span class="p">)</span>
<span class="n">rvs_2</span> <span class="o">=</span> <span class="nf">sp_uniform</span><span class="p">(</span><span class="mf">0.6</span><span class="p">,</span><span class="mf">0.4</span><span class="p">)</span>
<span class="n">cv_params_rd3</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">subsample</span><span class="sh">"</span><span class="p">:</span> <span class="n">rvs_1</span><span class="p">,</span>
                 <span class="sh">"</span><span class="s">colsample_bytree</span><span class="sh">"</span><span class="p">:</span> <span class="n">rvs_2</span><span class="p">}</span>

<span class="c1"># randomized search
</span><span class="n">n_iter_search</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">optimized_XGB</span> <span class="o">=</span> <span class="nc">RandomizedSearchCV</span><span class="p">(</span><span class="n">xgb</span><span class="p">.</span><span class="nc">XGBRegressor</span><span class="p">(</span><span class="o">**</span><span class="n">fixed_params_rd3</span><span class="p">),</span>
                               <span class="n">cv_params_rd3</span><span class="p">,</span>
                               <span class="n">scoring</span> <span class="o">=</span> <span class="sh">'</span><span class="s">neg_mean_squared_error</span><span class="sh">'</span><span class="p">,</span>
                               <span class="n">cv</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
                               <span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter_search</span><span class="p">,</span>
                               <span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
                               <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>


<span class="n">start</span> <span class="o">=</span> <span class="nf">time</span><span class="p">()</span>

<span class="n">optimized_XGB</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">ylog_train</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">RandomizedSearchCV took %.2f seconds for %d candidates</span><span class="sh">"</span>
      <span class="sh">"</span><span class="s"> parameter settings.</span><span class="sh">"</span> <span class="o">%</span> <span class="p">((</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">),</span> <span class="n">n_iter_search</span><span class="p">))</span>
<span class="nf">report</span><span class="p">(</span><span class="n">optimized_XGB</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">)</span>

<span class="c1"># predict the log(1+p)
</span><span class="n">ylog_pred</span> <span class="o">=</span> <span class="n">optimized_XGB</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># convert to price
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">ylog_pred</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

<span class="c1"># create pandas dataframe
</span><span class="n">df_sub</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">:</span> <span class="n">id_test</span><span class="p">,</span> <span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">:</span> <span class="n">y_pred</span><span class="p">})</span>
<span class="c1"># write csv file
</span><span class="n">df_sub</span><span class="p">.</span><span class="nf">to_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">sub_xgb_rd3.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">optimized_XGB</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">ylog_val</span><span class="p">)</span>
<span class="k">print</span> <span class="sh">"</span><span class="s">Score on the validation set (~0.30 of training set) = %.5f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">score</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Fitting 5 folds for each of 50 candidates, totalling 250 fits


[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 14.9min
[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed: 61.8min
[Parallel(n_jobs=4)]: Done 250 out of 250 | elapsed: 81.2min finished


RandomizedSearchCV took 4929.91 seconds for 50 candidates parameter settings.
Model with rank: 1
Mean validation score: -0.21567 (std: 0.00755)
Parameters: {'subsample': 0.91681240477025272, 'colsample_bytree': 0.63109952938936653}

Model with rank: 2
Mean validation score: -0.21580 (std: 0.00721)
Parameters: {'subsample': 0.97493920653568367, 'colsample_bytree': 0.86146309470589277}

Model with rank: 3
Mean validation score: -0.21602 (std: 0.00741)
Parameters: {'subsample': 0.97527816500403364, 'colsample_bytree': 0.73918613980754966}

Score on the validation set (~0.30 of training set) = 0.41667
</code></pre></div></div> <p>The public score for this step is 0.34924. Both the public and R2 scores are not significantly different than those from the previous step. Both are still an improvement over the random forest baseline.</p> <h4 id="step-4-tune-regularization-parameters">Step 4: Tune regularization parameters</h4> <p>The hyperparameters <code class="language-plaintext highlighter-rouge">reg_alpha</code> and <code class="language-plaintext highlighter-rouge">reg_lambda</code> are the <a href="http://cs.nyu.edu/~rostami/presentations/L1_vs_L2.pdf" rel="external nofollow noopener" target="_blank">L1 and L2 regularization terms</a>, respectively. The function <code class="language-plaintext highlighter-rouge">GridSearchCV</code> is used due to the scale desired on the L1 term <code class="language-plaintext highlighter-rouge">reg_alpha</code>. It would be difficult to randomly sample and hit the desired scales to test for this hyperparameter.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fixed_params_rd4</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">n_estimators</span><span class="sh">'</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">subsample</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.917</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">colsample_bytree</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.631</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">max_depth</span><span class="sh">"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">min_child_weight</span><span class="sh">"</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">gamma</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.122</span>
<span class="p">}</span>

<span class="c1"># specify parameters to search across
</span><span class="n">cv_params_rd4</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">reg_alpha</span><span class="sh">'</span><span class="p">:[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">],</span>
                 <span class="sh">'</span><span class="s">reg_lambda</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]}</span>

<span class="c1"># grid search
</span><span class="n">optimized_XGB</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="n">xgb</span><span class="p">.</span><span class="nc">XGBRegressor</span><span class="p">(</span><span class="o">**</span><span class="n">fixed_params_rd4</span><span class="p">),</span>
                         <span class="n">cv_params_rd4</span><span class="p">,</span>
                         <span class="n">scoring</span> <span class="o">=</span> <span class="sh">'</span><span class="s">neg_mean_squared_error</span><span class="sh">'</span><span class="p">,</span>
                         <span class="n">cv</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
                         <span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
                         <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>


<span class="n">start</span> <span class="o">=</span> <span class="nf">time</span><span class="p">()</span>

<span class="n">optimized_XGB</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">ylog_train</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">GridSearchCV took %.2f seconds for %d candidates</span><span class="sh">"</span>
      <span class="sh">"</span><span class="s"> parameter settings.</span><span class="sh">"</span> <span class="o">%</span> <span class="p">((</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">),</span> <span class="mi">25</span><span class="p">))</span> <span class="c1">#25 = number of reg_alpha * number of reg_lambda entries
</span>
<span class="nf">report</span><span class="p">(</span><span class="n">optimized_XGB</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">)</span>

<span class="c1"># predict the log(1+p)
</span><span class="n">ylog_pred</span> <span class="o">=</span> <span class="n">optimized_XGB</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># convert to price
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">ylog_pred</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

<span class="c1"># create pandas dataframe
</span><span class="n">df_sub</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">:</span> <span class="n">id_test</span><span class="p">,</span> <span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">:</span> <span class="n">y_pred</span><span class="p">})</span>
<span class="c1"># write csv file
</span><span class="n">df_sub</span><span class="p">.</span><span class="nf">to_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">sub_xgb_rd4.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">optimized_XGB</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">ylog_val</span><span class="p">)</span>
<span class="k">print</span> <span class="sh">"</span><span class="s">Score on the validation set (~0.30 of training set) = %.5f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">score</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Fitting 5 folds for each of 25 candidates, totalling 125 fits


[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 11.2min
[Parallel(n_jobs=4)]: Done 125 out of 125 | elapsed: 32.3min finished


GridSearchCV took 1996.67 seconds for 25 candidates parameter settings.
Model with rank: 1
Mean validation score: -0.21598 (std: 0.00720)
Parameters: {'reg_alpha': 0.01, 'reg_lambda': 1.0}

Model with rank: 2
Mean validation score: -0.21608 (std: 0.00720)
Parameters: {'reg_alpha': 0.005, 'reg_lambda': 0.7}

Model with rank: 3
Mean validation score: -0.21608 (std: 0.00678)
Parameters: {'reg_alpha': 0.001, 'reg_lambda': 0.8}

Score on the validation set (~0.30 of training set) = 0.41689
</code></pre></div></div> <p>The public score for this step is 0.35830. This is a more significant difference than those observed in previous steps, and therefore this step is not implemented in the final solution.</p> <h4 id="step-5-tune-learning_rate">Step 5: Tune <code class="language-plaintext highlighter-rouge">learning_rate</code> </h4> <p>One potential <a href="http://machinelearningmastery.com/tune-learning-rate-for-gradient-boosting-with-xgboost-in-python/" rel="external nofollow noopener" target="_blank">issue</a> with XGBoost, and boosting algorithms in general, is that the model can learn too quickly and overfit to the training data. Therefore, it is desirable to slow the learners and prevent this overfitting. The hyperparameter <code class="language-plaintext highlighter-rouge">learning_rate</code> (though it is called <code class="language-plaintext highlighter-rouge">eta</code> in the xgboost package) represents this learning rate.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fixed_params_rd5</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">n_estimators</span><span class="sh">'</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">subsample</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.917</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">colsample_bytree</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.631</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">max_depth</span><span class="sh">"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">min_child_weight</span><span class="sh">"</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">gamma</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.122</span>
<span class="p">}</span>

<span class="c1"># specify parameters to search across
</span><span class="n">rvs</span> <span class="o">=</span> <span class="nf">sp_uniform</span><span class="p">(.</span><span class="mi">00001</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">)</span>
<span class="n">cv_params_rd5</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">learning_rate</span><span class="sh">'</span><span class="p">:</span> <span class="n">rvs</span><span class="p">}</span>

<span class="c1"># randomized search
</span><span class="n">n_iter_search</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">optimized_XGB</span> <span class="o">=</span> <span class="nc">RandomizedSearchCV</span><span class="p">(</span><span class="n">xgb</span><span class="p">.</span><span class="nc">XGBRegressor</span><span class="p">(</span><span class="o">**</span><span class="n">fixed_params_rd5</span><span class="p">),</span>
                               <span class="n">cv_params_rd5</span><span class="p">,</span>
                               <span class="n">scoring</span> <span class="o">=</span> <span class="sh">'</span><span class="s">neg_mean_squared_error</span><span class="sh">'</span><span class="p">,</span>
                               <span class="n">cv</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
                               <span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter_search</span><span class="p">,</span>
                               <span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
                               <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>


<span class="n">start</span> <span class="o">=</span> <span class="nf">time</span><span class="p">()</span>

<span class="n">optimized_XGB</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">ylog_train</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">RandomizedSearchCV took %.2f seconds for %d candidates</span><span class="sh">"</span>
      <span class="sh">"</span><span class="s"> parameter settings.</span><span class="sh">"</span> <span class="o">%</span> <span class="p">((</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">),</span> <span class="n">n_iter_search</span><span class="p">))</span>

<span class="nf">report</span><span class="p">(</span><span class="n">optimized_XGB</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">)</span>

<span class="c1"># predict the log(1+p)
</span><span class="n">ylog_pred</span> <span class="o">=</span> <span class="n">optimized_XGB</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># convert to price
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">ylog_pred</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

<span class="c1"># create pandas dataframe
</span><span class="n">df_sub</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">:</span> <span class="n">id_test</span><span class="p">,</span> <span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">:</span> <span class="n">y_pred</span><span class="p">})</span>
<span class="c1"># write csv file
</span><span class="n">df_sub</span><span class="p">.</span><span class="nf">to_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">sub_xgb_rd5.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">optimized_XGB</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">ylog_val</span><span class="p">)</span>
<span class="k">print</span> <span class="sh">"</span><span class="s">Score on the validation set (~0.30 of training set) = %.5f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">score</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Fitting 5 folds for each of 20 candidates, totalling 100 fits


[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 10.5min
[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed: 24.0min finished


RandomizedSearchCV took 1498.82 seconds for 20 candidates parameter settings.
Model with rank: 1
Mean validation score: -0.21433 (std: 0.00704)
Parameters: {'learning_rate': 0.059702314218126537}

Model with rank: 2
Mean validation score: -0.21444 (std: 0.00721)
Parameters: {'learning_rate': 0.067640916284784261}

Model with rank: 3
Mean validation score: -0.21529 (std: 0.00744)
Parameters: {'learning_rate': 0.077684399299692578}

Score on the validation set (~0.30 of training set) = 0.42211
</code></pre></div></div> <p>The public score is 0.33300. This is an improvement in both public score and R2! In fact, this is better than any step so far, which is the ultimate goal.</p> <h4 id="step-6--tune-n_estimators">Step 6: Tune <code class="language-plaintext highlighter-rouge">n_estimators</code> </h4> <p>The hyperparameter <code class="language-plaintext highlighter-rouge">n_estimators</code> is the number of trees to fit. The <code class="language-plaintext highlighter-rouge">GridSearchCV</code> function is used here due to the scale desired to test.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fixed_params_rd6</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">learning_rate</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.0597</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">subsample</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.917</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">colsample_bytree</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.631</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">max_depth</span><span class="sh">"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">min_child_weight</span><span class="sh">"</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">gamma</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.122</span>
<span class="p">}</span>

<span class="c1"># specify parameters to search across
</span><span class="n">cv_params_rd6</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">n_estimators</span><span class="sh">'</span><span class="p">:[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">5000</span><span class="p">]}</span>

<span class="c1"># grid search
</span><span class="n">optimized_XGB</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="n">xgb</span><span class="p">.</span><span class="nc">XGBRegressor</span><span class="p">(</span><span class="o">**</span><span class="n">fixed_params_rd6</span><span class="p">),</span>
                         <span class="n">cv_params_rd6</span><span class="p">,</span>
                         <span class="n">scoring</span> <span class="o">=</span> <span class="sh">'</span><span class="s">neg_mean_squared_error</span><span class="sh">'</span><span class="p">,</span>
                         <span class="n">cv</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
                         <span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
                         <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>


<span class="n">start</span> <span class="o">=</span> <span class="nf">time</span><span class="p">()</span>

<span class="n">optimized_XGB</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">ylog_train</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">GridSearchCV took %.2f seconds for %d candidates</span><span class="sh">"</span>
      <span class="sh">"</span><span class="s"> parameter settings.</span><span class="sh">"</span> <span class="o">%</span> <span class="p">((</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">),</span> <span class="mi">5</span><span class="p">))</span> 

<span class="nf">report</span><span class="p">(</span><span class="n">optimized_XGB</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">)</span>

<span class="c1"># predict the log(1+p)
</span><span class="n">ylog_pred</span> <span class="o">=</span> <span class="n">optimized_XGB</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># convert to price
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">ylog_pred</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

<span class="c1"># create pandas dataframe
</span><span class="n">df_sub</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">:</span> <span class="n">id_test</span><span class="p">,</span> <span class="sh">'</span><span class="s">price_doc</span><span class="sh">'</span><span class="p">:</span> <span class="n">y_pred</span><span class="p">})</span>
<span class="c1"># write csv file
</span><span class="n">df_sub</span><span class="p">.</span><span class="nf">to_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">sub_xgb_rd6.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">optimized_XGB</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">ylog_val</span><span class="p">)</span>
<span class="k">print</span> <span class="sh">"</span><span class="s">Score on the validation set (~0.30 of training set) = %.5f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">score</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Fitting 5 folds for each of 5 candidates, totalling 25 fits


[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed: 21.8min finished


GridSearchCV took 1368.02 seconds for 5 candidates parameter settings.
Model with rank: 1
Mean validation score: -0.21448 (std: 0.00700)
Parameters: {'n_estimators': 500}

Model with rank: 2
Mean validation score: -0.21665 (std: 0.00743)
Parameters: {'n_estimators': 1000}

Model with rank: 3
Mean validation score: -0.22291 (std: 0.00670)
Parameters: {'n_estimators': 100}

Score on the validation set (~0.30 of training set) = 0.42211
</code></pre></div></div> <p>The optimal number of estimators appears to be 500, as we’ve been using all along. Since the results of this step are equivalent to those of Step 5, the public score is not reported.</p> <h2 id="iv-results">IV. Results</h2> <h3 id="model-evaluation-and-validation">Model Evaluation and Validation</h3> <p>The process outlined above for arriving at a final model is reasonable; model hyperparameters were chosen at each step that improve R2 and public leaderboard scores over a benchmark model. However, a single validation set was used in the tuning. Now, it is necessary to test for generalization to unseen data; that is, using multiple training and validation sets and comparing model performance across these multiple sets. This can be done using <a href="http://scikit-learn.org/stable/modules/cross_validation.html" rel="external nofollow noopener" target="_blank">cross-validation</a> (<a href="https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/" rel="external nofollow noopener" target="_blank">here</a> is another useful link). The package <code class="language-plaintext highlighter-rouge">sklearn.model_selection</code> has two utilities that will be needed to do this analysis:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">learning_curve</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">ShuffleSplit</span>
</code></pre></div></div> <p>The function <code class="language-plaintext highlighter-rouge">learning_curve</code> computes R2 scores for training and validation scores across a cross-validation generator. The function <code class="language-plaintext highlighter-rouge">ShuffleSplit</code> creates cross-validation datasets based upon the desired number of instances, train-validation split ratio, and a random seed. It would be useful to have a <a href="http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py" rel="external nofollow noopener" target="_blank">helper function</a> to do the cross-validation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_learning_curve</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                        <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">train_sizes</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(.</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                        <span class="n">fname</span><span class="o">=</span><span class="sh">"</span><span class="s">output.png</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Generate a simple plot of the test and training learning curve.

    Parameters
    ----------
    estimator : object type that implements the </span><span class="sh">"</span><span class="s">fit</span><span class="sh">"</span><span class="s"> and </span><span class="sh">"</span><span class="s">predict</span><span class="sh">"</span><span class="s"> methods
        An object of that type which is cloned for each validation.

    title : string
        Title for the chart.

    X : array-like, shape (n_samples, n_features)
        Training vector, where n_samples is the number of samples and
        n_features is the number of features.

    y : array-like, shape (n_samples) or (n_samples, n_features), optional
        Target relative to X for classification or regression;
        None for unsupervised learning.

    ylim : tuple, shape (ymin, ymax), optional
        Defines minimum and maximum yvalues plotted.

    cv : int, cross-validation generator or an iterable, optional
        Determines the cross-validation splitting strategy.
        Possible inputs for cv are:
          - None, to use the default 3-fold cross-validation,
          - integer, to specify the number of folds.
          - An object to be used as a cross-validation generator.
          - An iterable yielding train/test splits.

        For integer/None inputs, if ``y`` is binary or multiclass,
        :class:`StratifiedKFold` used. If the estimator is not a classifier
        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.

        Refer :ref:`User Guide &lt;cross_validation&gt;` for the various
        cross-validators that can be used here.

    n_jobs : integer, optional
        Number of jobs to run in parallel (default 1).
    </span><span class="sh">"""</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ylim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="o">*</span><span class="n">ylim</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Training examples</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Score</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">train_sizes</span><span class="p">,</span> <span class="n">train_scores</span><span class="p">,</span> <span class="n">test_scores</span> <span class="o">=</span> <span class="nf">learning_curve</span><span class="p">(</span>
        <span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">train_sizes</span><span class="o">=</span><span class="n">train_sizes</span><span class="p">)</span>
    <span class="n">train_scores_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">train_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">train_scores_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">train_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_scores_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">test_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_scores_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">test_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">()</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="n">train_sizes</span><span class="p">,</span> <span class="n">train_scores_mean</span> <span class="o">-</span> <span class="n">train_scores_std</span><span class="p">,</span>
                     <span class="n">train_scores_mean</span> <span class="o">+</span> <span class="n">train_scores_std</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                     <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="n">train_sizes</span><span class="p">,</span> <span class="n">test_scores_mean</span> <span class="o">-</span> <span class="n">test_scores_std</span><span class="p">,</span>
                     <span class="n">test_scores_mean</span> <span class="o">+</span> <span class="n">test_scores_std</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">g</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">train_sizes</span><span class="p">,</span> <span class="n">train_scores_mean</span><span class="p">,</span> <span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">,</span>
             <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Training score</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">train_sizes</span><span class="p">,</span> <span class="n">test_scores_mean</span><span class="p">,</span> <span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">g</span><span class="sh">"</span><span class="p">,</span>
             <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Cross-validation score</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="sh">"</span><span class="s">best</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">plt</span>
</code></pre></div></div> <p>Now, apply this function using the tuned regressor constructed in the previous section.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">title</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Learning Curves (XGBRegressor)</span><span class="sh">"</span>
<span class="c1"># Cross validation with 100 iterations to get smoother mean test and train
# score curves, each time with 20% data randomly selected as a validation set.
</span><span class="n">cv</span> <span class="o">=</span> <span class="nc">ShuffleSplit</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">estimator</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="nc">XGBRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> 
                             <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0597</span><span class="p">,</span>
                             <span class="n">subsample</span><span class="o">=</span><span class="mf">0.917</span><span class="p">,</span>
                             <span class="n">colsample_bytree</span><span class="o">=</span><span class="mf">0.631</span><span class="p">,</span>
                             <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                             <span class="n">max_depth</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
                             <span class="n">gamma</span><span class="o">=</span><span class="mf">0.122</span><span class="p">,</span>
                             <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                             <span class="n">silent</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="nf">plot_learning_curve</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">ylog_train</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">fname</span><span class="o">=</span><span class="sh">"</span><span class="s">output_images/xgb_cv.png</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;module 'matplotlib.pyplot' from '/Users/jdinius/miniconda2/envs/ml_capstone/lib/python2.7/site-packages/matplotlib/pyplot.pyc'&gt;
</code></pre></div></div> <p><img src="/assets/img/capstone_1_files/capstone_1_133_1.png" alt="png"></p> <p>The two mean scores lie directly on top of one another, indicating that the performance on the training and validation sets matches very closely, nearly perfectly, in fact. Any error would be well-within the 1-sigma uncertainty bounds, indicated by the green solid area bisected by the mean scores. It can be concluded that the model generalizes very well to unseen data. The data appear to be approaching an asymptote, however this asymptote appears to be less than 0.5 even, indicating that housing price is not fully predictable given the feature set used. The plot also indicates that performance could be improved with more training examples.</p> <h3 id="justification">Justification</h3> <p>To justify the use of this model, a simple comparison to the benchmark can be made. A good place to start would be to repeat the above cross-validation on the benchmark model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">title</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Learning Curves (RandomForestRegressor)</span><span class="sh">"</span>
<span class="c1"># Cross validation with 100 iterations to get smoother mean test and train
# score curves, each time with 20% data randomly selected as a validation set.
</span><span class="n">cv</span> <span class="o">=</span> <span class="nc">ShuffleSplit</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">estimator</span> <span class="o">=</span> <span class="nc">RandomForestRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="nf">plot_learning_curve</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">ylog_train</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">fname</span><span class="o">=</span><span class="sh">"</span><span class="s">output_images/rgr_cv.png</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;module 'matplotlib.pyplot' from '/Users/jdinius/miniconda2/envs/ml_capstone/lib/python2.7/site-packages/matplotlib/pyplot.pyc'&gt;
</code></pre></div></div> <p><img src="/assets/img/capstone_1_files/capstone_1_137_1.png" alt="png"></p> <p>The plot above shows subtle differences from the cross-validation plot for the tuned regressor. First, there are some similarities. Most notably, this regressor would perform better with more training results. The asymptotic value that the data appears to be approaching is lower than that of the tuned regressor plot; the slope of the mean score line is flatter in this plot. It appears as though maximum value of the R2 score for this regressor will occur for a smaller training sample size than the maximum obtained for the tuned regressor. As in the tuned regressor case, the model generalizes well to unseen data.</p> <p>The final solution is significant enough to solve the problem at hand. The model gives reasonable performance given the complexity of the data and the minimal addition of features extracted from the raw data. Practically, there is only so much training data that is available. If more data were available, the model performance would be a little better, but nowhere near 1, as indicated by the cross-validation plot. This solution is favorable over more complex solutions (such as stacked regressors) since it is easy to implement, easy to interpret, and provides adequate performance.</p> <h2 id="v-conclusion">V. Conclusion</h2> <h3 id="free-form-visualization">Free-Form Visualization</h3> <p>There is one table that hasn’t been shown yet, but summarizes the analysis quite well.</p> <table> <thead> <tr> <th><strong>Regressor</strong></th> <th><strong>R2 score</strong></th> <th><strong>Improvement</strong></th> <th><strong>Kaggle Public Score</strong></th> <th><strong>Improvement</strong></th> </tr> </thead> <tbody> <tr> <td>Random Forest</td> <td>0.39009</td> <td>N/A</td> <td>0.41168</td> <td>N/A</td> </tr> <tr> <td>XGB Baseline</td> <td>0.40971</td> <td>+0.050</td> <td>0.39276</td> <td>-0.046</td> </tr> <tr> <td>XGB Step 1</td> <td>0.41860</td> <td>+0.069</td> <td>0.34795</td> <td>-0.155</td> </tr> <tr> <td>XGB Step 2</td> <td>0.41756</td> <td>+0.071</td> <td>0.34814</td> <td>-0.154</td> </tr> <tr> <td>XGB Step 3</td> <td>0.41667</td> <td>+0.068</td> <td>0.34924</td> <td>-0.152</td> </tr> <tr> <td>XGB Step 4</td> <td>0.41689</td> <td>+0.069</td> <td>0.35830</td> <td>-0.130</td> </tr> <tr> <td>XGB Step 5</td> <td>0.42211</td> <td>+0.082</td> <td>0.33300</td> <td>-0.191</td> </tr> <tr> <td>XGB Step 6 (Final)</td> <td>0.42211</td> <td>+0.082</td> <td>0.33300</td> <td>-0.191</td> </tr> </tbody> </table> <p>The above table shows improvement of each step over the benchmark. The <strong>Improvement</strong> columns show the results of the relative improvement over the benchmark, computed as <code class="language-plaintext highlighter-rouge">(step*_score-benchmark_score)/benchmark_score</code>. These columns can be interpreted, after multiplying by 100, as the percent relative change in the scores from the benchmark compared to each step. The sought after trends are a positive change in R2 score and a negative change in the Kaggle public score. The table shows that, through tuning, an 8% positive change in R2 score, along with a large 19% relative change in public score, were achieved.</p> <h3 id="reflection">Reflection</h3> <p>A model was developed for predicting the sales price of properties within the Russian housing market based upon features provided by Sberbank, a Russian bank. Plots were presented, and analyzed, showing trends in both micro- and macroeconomic features, and the relationship of these features to sales price. Minimal feature engineering was performed on the dataset to provide a few additional, potentially important features. A benchmark model based on an untuned random forest regressor was presented as an initial first cut at a solution. Based upon previous analyses and their results, XGBoost was selected, and tuned, as the final solution for this problem. XGBoost provided a nice framework to build upon since it requires minimal feature engineering and scaling to achieve best results.</p> <p>Quite surprising was the fact that PCA didn’t really improve performance, despite the high degree of multicollinearity observed in the data. Perhaps naively, one would think that creating a new pseudofeature set that orthogonally spans the feature space would allow for greater predictive power of the underlying model. However, the results seem to confirm the previous notion that decision trees are <a href="https://stats.stackexchange.com/questions/141864/how-can-top-principal-components-retain-the-predictive-power-on-a-dependent-vari" rel="external nofollow noopener" target="_blank">insensitive to principal component reduction</a>.</p> <p>This project presented some difficulties in interpreting the data provided. There were many features, and many of these features seemed very interrelated. There was also much data missing. Fortunately, there are good standalone packages within <code class="language-plaintext highlighter-rouge">sklearn</code> for dealing with such cases. In the end, it was great that there exists a simple-to-use framework, XGBoost, that made working with the data very easy.</p> <p>The final model fits expectations, and the process followed for tuning was very straightforward. A similar process could be used for solving problems of a similar type: data analysis, data reduction/scaling, feature engineering, and regressor development are the common steps for any regression analysis. Depending upon the data involved, it may not be necessary to use as strong a learner as XGBoost; a simple linear regressor may be adequate if the degree of nonlinearity in the target variable is low. Given the data observed in the cross-validation plots and the amount of training data available, it seems that this model performs about as well as can be expected for a single regressor approach.</p> <h3 id="improvement">Improvement</h3> <p>It would seem that better performance might be achieved if multiple learners were combined. Some algorithms may achieve better performance for some regions in the feature space while others might perform better elsewhere. The package <a href="https://github.com/rasbt/mlxtend" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">mlxtend</code></a> provides a Python framework for doing stacked regression. Stacked regression provides a way of combining multiple regressors into a single predictor, and has been shown to <a href="http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/" rel="external nofollow noopener" target="_blank">work well in other Kaggle competitions</a>. There is a trade-off here, though. Typically, the added complexity comes with relatively modest improvements. This added complexity may obscure any interpretation of the model; functional relationships between features and target variables may be more difficult to establish despite any observed increases in predictive power (i.e. R2 score).</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Joe Dinius, Ph.D.. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>