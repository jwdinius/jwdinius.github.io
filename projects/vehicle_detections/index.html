<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Single Shot Detection &amp; Tracking | Joe Dinius, Ph.D.</title> <meta name="author" content="Joe Dinius, Ph.D."> <meta name="description" content="OpenCV, SVM, Kalman filter"> <meta name="keywords" content="robotics, autonomy, math, optimization, controltheory, planning, computervision"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?df44dccb15554b4d2d173cb203488555"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?53ca2be8c4ec0d533ef79017d1a2d734"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jwdinius.github.io/projects/vehicle_detections/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Joe </span>Dinius, Ph.D.</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">git</a> </li> <li class="nav-item "> <a class="nav-link" href="/learning/">learning</a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/resumeDiniusTargeted.pdf">resume</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Single Shot Detection &amp; Tracking</h1> <p class="post-description">OpenCV, SVM, Kalman filter</p> </header> <article> <h2 id="abstract">Abstract</h2> <p>A single shot detector is built to identify cars within a given video stream. The output of the detector is <em>minimal</em> bounding boxes around detected cars. Bounding box transients are smoothed using a Kalman filter tracker implemented in pixel space.</p> <h2 id="outline">Outline</h2> <p>This project is broken into the following steps:</p> <ul> <li>Perform feature extraction on a given labeled training set of images, aka preprocessing</li> <li>Train a classifier</li> <li>Perform sliding-window search and use the trained classifier to detect vehicles in images.</li> <li>Run the full processing pipeline on a video stream and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.</li> <li>Estimate a bounding box for vehicles detected.</li> </ul> <p>Here’s a link to the video that will be processed:</p> <p><a href="https://www.youtube.com/watch?v=JuBVYVb2Qc8" target="_blank" rel="external nofollow noopener"><img src="https://img.youtube.com/vi/JuBVYVb2Qc8/0.jpg" alt="raw"></a></p> <p>For all project materials, please see this <a href="https://github.com/jwdinius/CarND-Term1/tree/master/CarND-Vehicle-Detection" rel="external nofollow noopener" target="_blank">GitHub repo</a>.</p> <h2 id="preprocessing">Preprocessing</h2> <h3 id="histogram-of-oriented-gradients">Histogram of Oriented Gradients</h3> <p>For object detection within images, Histogram of Oriented Gradients is a powerful technique. The gist of HOG is that object shape can be extracted from the distribution of intensity gradients or edge directions, hence the <em>orientation</em> piece. I’ll outline the steps below for implementing a HOG feature extraction for object detection.</p> <p>The code for this step is contained in the function <code class="language-plaintext highlighter-rouge">get_hog_features</code> (or in lines 6 through 24 of the file called <code class="language-plaintext highlighter-rouge">lesson_functions.py</code>). The call to this function is in <code class="language-plaintext highlighter-rouge">extract_features</code> (lines 45-95 of <code class="language-plaintext highlighter-rouge">lesson_functions.py</code>). The function <code class="language-plaintext highlighter-rouge">extract_features</code> is called in the main routine (lines 41-46 in <code class="language-plaintext highlighter-rouge">classify.py</code>).</p> <p>Before getting into the construction of feature vectors, though, some preliminary steps were taken:</p> <p>I started by reading in all the <code class="language-plaintext highlighter-rouge">vehicle</code> and <code class="language-plaintext highlighter-rouge">non-vehicle</code> images. Here is an example of one of each of the <code class="language-plaintext highlighter-rouge">vehicle</code> and <code class="language-plaintext highlighter-rouge">non-vehicle</code> classes:</p> <p><img src="/assets/img/vehicle_detection/car_notcar.png" alt="car_notcar"></p> <p>I then explored different color spaces and different <code class="language-plaintext highlighter-rouge">skimage.feature.hog()</code> parameters (<code class="language-plaintext highlighter-rouge">orientations</code>, <code class="language-plaintext highlighter-rouge">pixels_per_cell</code>, and <code class="language-plaintext highlighter-rouge">cells_per_block</code>). I grabbed random images (random since the paths were shuffled using the <code class="language-plaintext highlighter-rouge">sklearn.model_selection.train_test_split</code> function) from each of the two classes and displayed them to get a feel for what the <code class="language-plaintext highlighter-rouge">skimage.feature.hog()</code> output looks like.</p> <p>Here is an example using the <code class="language-plaintext highlighter-rouge">HLS</code> color space and HOG parameters of <code class="language-plaintext highlighter-rouge">orientations=9</code>, <code class="language-plaintext highlighter-rouge">pix_per_cell=(8, 8)</code> and <code class="language-plaintext highlighter-rouge">cells_per_block=(2, 2)</code>:</p> <p><img src="/assets/img/vehicle_detection/HOG_features_HLS.png" alt="HLS"></p> <p>I tried various combinations of parameters and settled on those which, along with the chosen classifier, gave a large accuracy on both the training and validation sets. For me, I thought that accuracies within 1% of each other above 97% was sufficient. For the final parameter set chosen, see lines 28-37 in <code class="language-plaintext highlighter-rouge">classify.py</code>.</p> <h2 id="training-a-classifier">Training a Classifier</h2> <p>I trained a linear SVM using default parameters (see <a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html" rel="external nofollow noopener" target="_blank">this</a>). For the specific implementation, see lines 74-80 in <code class="language-plaintext highlighter-rouge">classify.py</code>. The linear SVM was chosen over higher-order kernel methods since it provided sufficient accuracy on both the training and validation data sets while minimizing classification time; about 98%.</p> <h2 id="sliding-window-search">Sliding Window Search</h2> <p>Since we only really care about vehicles below the horizon, I minimized the amount of y pixels to be searched over. After much trial-and-error, values were chosen that gave decent detection performance on images within the <code class="language-plaintext highlighter-rouge">test_images</code> directory (see line 52-58 of <code class="language-plaintext highlighter-rouge">searcher.py</code> in the <code class="language-plaintext highlighter-rouge">search_all_scales</code> function. The implementation of the sliding window search algorithm is contained within the <code class="language-plaintext highlighter-rouge">slide_window</code> function (see lines 101-143 in <code class="language-plaintext highlighter-rouge">lesson_functions.py</code>). The gist of the algorithm is to create windows based upon desired overlap and pixel positions and then append, and subsequently return, a list containing valid windows to search for detections across. This is the first part of the process. The second part is contained within the <code class="language-plaintext highlighter-rouge">search_all_scales</code> function within <code class="language-plaintext highlighter-rouge">searcher.py</code> (lines 47-76). In this routine, “hot” windows are identified by a call to <code class="language-plaintext highlighter-rouge">search_windows</code>, which uses the linear svm classifier trained previously to determine whether or not a car detection was made within that window. The definition of <code class="language-plaintext highlighter-rouge">search_windows</code> is in <code class="language-plaintext highlighter-rouge">lesson_functions.py</code> (lines 209-238). For an example of output on test images from this approach, see the following:</p> <p><img src="/assets/img/vehicle_detection/sliding_windows.png" alt="swind"></p> <h2 id="putting-it-all-together">Putting it All Together</h2> <p>After trial over multiple scales, and different overlap ratios, I ultimately searched on 4 scales using HLS 3-channel HOG features plus spatially binned color and histograms of color in the feature vector. The feature vector was then scaled using <code class="language-plaintext highlighter-rouge">sklearn.preprocessing.StandardScaler()</code> (see lines 52-57 of <code class="language-plaintext highlighter-rouge">classify.py</code>) to ensure appropriate scaling. Here are some example images:</p> <p><img src="/assets/img/vehicle_detection/detection_example.png" alt="tog"></p> <p>There are clearly some issues with false positives. To address these issues, I recorded the positions of positive detections in each frame of the video. From the positive detections I created a heatmap and then thresholded that map to identify vehicle positions. I then used <code class="language-plaintext highlighter-rouge">scipy.ndimage.measurements.label()</code> to identify individual blobs in the heatmap. Assuming each blob corresponded to a vehicle, I constructed bounding boxes to cover the area of each blob detected.</p> <p>Here’s an example result showing the heatmap from a series of frames of video, the result of <code class="language-plaintext highlighter-rouge">scipy.ndimage.measurements.label()</code> and the bounding boxes then overlaid on a frame within the video:</p> <p>IMPORTANT NOTE: I used the following commands, from within the project directory, to generate the heatmap directory and subsequent files needed for the plots below: <code class="language-plaintext highlighter-rouge">mkdir heatmap; ffmpeg -i project_video.mp4 -r 60/1 heatmap/output%03d.jpg</code></p> <p><img src="/assets/img/vehicle_detection/heatmap.png" alt="heatmap"></p> <p>Here is the output of <code class="language-plaintext highlighter-rouge">scipy.ndimage.measurements.label()</code> on the integrated heatmap from all six frames:</p> <p><img src="/assets/img/vehicle_detection/labels.png" alt="labels"></p> <p>Here the resulting bounding boxes are drawn onto the last frame in the series:</p> <p><img src="/assets/img/vehicle_detection/bounding_boxes.png" alt="boxes"></p> <h2 id="after-all-the-postprocessing">After All the Postprocessing…</h2> <p>Below you’ll find a link to the processed video (minus the tracker):</p> <p><a href="https://www.youtube.com/watch?v=VysM74ktGTE" target="_blank" rel="external nofollow noopener"><img src="https://img.youtube.com/vi/VysM74ktGTE/0.jpg" alt="detect"></a></p> <h2 id="tracking">Tracking</h2> <p>The detector performs reasonably well, but the bounding boxes are a little noisy. I next added a Kalman filter to smooth out the bounding boxes. The Kalman filter process model I chose, based upon how linear the motion of the vehicles seemed, is constant velocity with a constant aspect ratio. The constant aspect ratio seemed appropriate given that, as a vehicle moves towards the horizon, will scale smaller equally in both width and height.</p> <p>There was also the problem of data association: How do I pick which measurement associates to which track from frame-to-frame? I chose a simple, greedy algorithm that picks the measurement-track pair that yields the smallest normalized-innovation-squared at the prediction step. This is a pretty typical approach.</p> <p>Here’s a link to the full pipeline; detection + tracker:</p> <p><a href="https://www.youtube.com/watch?v=SPLXFGI71FE" target="_blank" rel="external nofollow noopener"><img src="https://img.youtube.com/vi/SPLXFGI71FE/0.jpg" alt="detect"></a></p> <hr> <h3 id="concluding-remarks">Concluding Remarks</h3> <p>There was a lot of trial-and-error trying to get decent performance on the videos; all tuning effort, after heatmap filtering, was focused on performance on the video. The results look pretty good.</p> <p>To make the pipeline more robust, I would like to have more time to investigate additional features to add for classification. All things considered, performance was pretty good with a quite limited feature set.</p> <p>From the video, it’s clear that that there are issues when two vehicles come within a single window search area. Individual vehicles are difficult to resolve in this case. Therefore, the pipeline will most likely have difficulty in high volume, crowded freeway and street traffic situations. There is a pretty good <a href="https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html" rel="external nofollow noopener" target="_blank">deep learning network architecture</a> that handles such simulations well on GPUs with low computational overhead. I’ll write a shorter post about this when I have a chance. It’s pretty cool so stay tuned!</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Joe Dinius, Ph.D.. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>